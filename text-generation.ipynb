{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585d72c0",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6324bbe",
   "metadata": {},
   "source": [
    "### Jos√© Pablo Kiesling Lange - 21581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6e1cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae80eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c860d9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TheKi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abe65dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabetic_chars(text):\n",
    "    return ''.join(char for char in text if char.isalpha() or char.isspace())\n",
    "\n",
    "def filter_ascii_words(text):\n",
    "    words = text.split()\n",
    "    ascii_words = [word for word in words if all(ord(char) < 128 for char in word)]\n",
    "    return ' '.join(ascii_words)\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = remove_non_alphabetic_chars(text)\n",
    "    text = filter_ascii_words(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    text = convert_to_lowercase(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab7bb6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_strings(text_list):\n",
    "    return [text for text in text_list if text.strip() != '']\n",
    "\n",
    "def add_special_tokens(text_list):\n",
    "    return ['<sos> ' + text + ' <eos>' for text in text_list]\n",
    "\n",
    "def create_token_sequences(text_list):\n",
    "    return [text.split() for text in text_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "106c3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset[\"train\"][\"text\"]\n",
    "dataset_test = dataset[\"test\"][\"text\"]\n",
    "dataset_validation = dataset[\"validation\"][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1086f432",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6b8829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(raw_texts):\n",
    "    normalized_texts = [normalize_text(text) for text in raw_texts]\n",
    "    filtered_texts = remove_empty_strings(normalized_texts)\n",
    "    return filtered_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf5bf093",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = preprocess_dataset(dataset_train)\n",
    "dataset_test = preprocess_dataset(dataset_test)\n",
    "dataset_validation = preprocess_dataset(dataset_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6516faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = ''.join(char for char in text if char.isalpha() or char.isspace())\n",
    "    text = ' '.join(word for word in text.split() if all(ord(char) < 128 for char in word))\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5ba8750",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = [normalize_text(text) for text in dataset_train if text.strip() != '']\n",
    "dataset_test = [normalize_text(text) for text in dataset_test if text.strip() != '']\n",
    "dataset_validation = [normalize_text(text) for text in dataset_validation if text.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47da89f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = [text for text in dataset_train if text != '']\n",
    "dataset_test = [text for text in dataset_test if text != '']\n",
    "dataset_validation = [text for text in dataset_validation if text != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4beab09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = add_special_tokens(dataset_train)\n",
    "dataset_test = add_special_tokens(dataset_test)\n",
    "dataset_validation = add_special_tokens(dataset_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6eb755bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ['<sos> ' + text + ' <eos>' for text in dataset_train]\n",
    "dataset_test = ['<sos> ' + text + ' <eos>' for text in dataset_test]\n",
    "dataset_validation = ['<sos> ' + text + ' <eos>' for text in dataset_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29c9f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = create_token_sequences(dataset_train)\n",
    "sequences_test = create_token_sequences(dataset_test)\n",
    "sequences_validation = create_token_sequences(dataset_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36670b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [token for sequence in sequences_train for token in sequence]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afde13e7",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5e3e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIALS = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
    "K = 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37f6d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = FreqDist(tok for seq in sequences_train for tok in seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35e70d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = SPECIALS + [tok for tok, _ in fd.most_common() if tok not in SPECIALS]\n",
    "stoi = {t: i for i, t in enumerate(itos)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82bd2b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_id(tok):\n",
    "    return stoi.get(tok, stoi[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e46577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_token(id):\n",
    "    return itos[id] if 0 <= id < len(itos) else \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edb8b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xy_from_sequences(seqs, k=5):\n",
    "    X, y = [], []\n",
    "    for seq in seqs:\n",
    "        for gram in ngrams(seq, k + 1):\n",
    "            ctx, tgt = gram[:-1], gram[-1]\n",
    "            X.append([to_id(t) for t in ctx])\n",
    "            y.append(to_id(tgt))\n",
    "    return np.array(X, dtype=np.int64), np.array(y, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05447b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 61031\n",
      "Train: X=(1621112, 5), y=(1621112,)\n",
      "Val:   X=(169743, 5),   y=(169743,)\n",
      "Test:  X=(190380, 5),  y=(190380,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = build_xy_from_sequences(sequences_train, K)\n",
    "X_val,   y_val   = build_xy_from_sequences(sequences_validation, K)\n",
    "X_test,  y_test  = build_xy_from_sequences(sequences_test, K)\n",
    "\n",
    "print(f\"Vocab size: {len(stoi)}\")\n",
    "print(f\"Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Val:   X={X_val.shape},   y={y_val.shape}\")\n",
    "print(f\"Test:  X={X_test.shape},  y={y_test.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowpy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
