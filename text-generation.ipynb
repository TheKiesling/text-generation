{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585d72c0",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6324bbe",
   "metadata": {},
   "source": [
    "### Jos√© Pablo Kiesling Lange - 21581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6e1cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TheKi\\OneDrive - UVG\\Semestre X\\Procesamiento de Lenguaje Natural\\text-generation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae80eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c860d9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TheKi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106c3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset[\"train\"][\"text\"]\n",
    "dataset_test = dataset[\"test\"][\"text\"]\n",
    "dataset_validation = dataset[\"validation\"][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1086f432",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6516faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = ''.join(char for char in text if char.isalpha() or char.isspace())\n",
    "    text = ' '.join(word for word in text.split() if all(ord(char) < 128 for char in word))\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5ba8750",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = [normalize_text(text) for text in dataset_train if text.strip() != '']\n",
    "dataset_test = [normalize_text(text) for text in dataset_test if text.strip() != '']\n",
    "dataset_validation = [normalize_text(text) for text in dataset_validation if text.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47da89f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = [text for text in dataset_train if text != '']\n",
    "dataset_test = [text for text in dataset_test if text != '']\n",
    "dataset_validation = [text for text in dataset_validation if text != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb755bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ['<sos> ' + text + ' <eos>' for text in dataset_train]\n",
    "dataset_test = ['<sos> ' + text + ' <eos>' for text in dataset_test]\n",
    "dataset_validation = ['<sos> ' + text + ' <eos>' for text in dataset_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29c9f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = [s.split() for s in dataset_train]\n",
    "sequences_test = [s.split() for s in dataset_test]\n",
    "sequences_validation = [s.split() for s in dataset_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36670b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [token for sequence in sequences_train for token in sequence]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afde13e7",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5e3e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIALS = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
    "K = 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37f6d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = FreqDist(tok for seq in sequences_train for tok in seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35e70d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = SPECIALS + [tok for tok, _ in fd.most_common() if tok not in SPECIALS]\n",
    "stoi = {t: i for i, t in enumerate(itos)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82bd2b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_id(tok):\n",
    "    return stoi.get(tok, stoi[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e46577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_token(id):\n",
    "    return itos[id] if 0 <= id < len(itos) else \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edb8b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xy_from_sequences(seqs, k=5):\n",
    "    X, y = [], []\n",
    "    for seq in seqs:\n",
    "        for gram in ngrams(seq, k + 1):\n",
    "            ctx, tgt = gram[:-1], gram[-1]\n",
    "            X.append([to_id(t) for t in ctx])\n",
    "            y.append(to_id(tgt))\n",
    "    return np.array(X, dtype=np.int64), np.array(y, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05447b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 61031\n",
      "Train: X=(1621112, 5), y=(1621112,)\n",
      "Val:   X=(169743, 5),   y=(169743,)\n",
      "Test:  X=(190380, 5),  y=(190380,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = build_xy_from_sequences(sequences_train, K)\n",
    "X_val,   y_val   = build_xy_from_sequences(sequences_validation, K)\n",
    "X_test,  y_test  = build_xy_from_sequences(sequences_test, K)\n",
    "\n",
    "print(f\"Vocab size: {len(stoi)}\")\n",
    "print(f\"Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Val:   X={X_val.shape},   y={y_val.shape}\")\n",
    "print(f\"Test:  X={X_test.shape},  y={y_test.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
