{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "author",
   "metadata": {},
   "source": [
    "### José Pablo Kiesling Lange - 21581"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nltk_download",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TheKi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_header",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "split_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset[\"train\"][\"text\"]\n",
    "dataset_test = dataset[\"test\"][\"text\"]\n",
    "dataset_validation = dataset[\"validation\"][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norm_header",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "norm_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabetic_chars(text):\n",
    "    return ''.join(char for char in text if char.isalpha() or char.isspace())\n",
    "\n",
    "def filter_ascii_words(text):\n",
    "    words = text.split()\n",
    "    ascii_words = [word for word in words if all(ord(char) < 128 for char in word)]\n",
    "    return ' '.join(ascii_words)\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = remove_non_alphabetic_chars(text)\n",
    "    text = filter_ascii_words(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    text = convert_to_lowercase(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "utils_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_strings(text_list):\n",
    "    return [text for text in text_list if text.strip() != '']\n",
    "\n",
    "def add_special_tokens(text_list):\n",
    "    return ['<sos> ' + text + ' <eos>' for text in text_list]\n",
    "\n",
    "def create_token_sequences(text_list):\n",
    "    return [text.split() for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "preprocess_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(raw_texts):\n",
    "    normalized_texts = [normalize_text(text) for text in raw_texts]\n",
    "    filtered_texts = remove_empty_strings(normalized_texts)\n",
    "    texts_with_tokens = add_special_tokens(filtered_texts)\n",
    "    return texts_with_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "apply_preprocess",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = preprocess_dataset(dataset_train)\n",
    "dataset_test = preprocess_dataset(dataset_test)\n",
    "dataset_validation = preprocess_dataset(dataset_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "create_sequences",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = create_token_sequences(dataset_train)\n",
    "sequences_test = create_token_sequences(dataset_test)\n",
    "sequences_validation = create_token_sequences(dataset_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "print_stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 23686\n",
      "Test samples: 2889\n",
      "Validation samples: 2454\n",
      "Sample sequence: ['<sos>', 'valkyria', 'chronicles', 'iii', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train samples: {len(sequences_train)}\")\n",
    "print(f\"Test samples: {len(sequences_test)}\")\n",
    "print(f\"Validation samples: {len(sequences_validation)}\")\n",
    "print(f\"Sample sequence: {sequences_train[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1_header",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Red Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocab_header",
   "metadata": {},
   "source": [
    "### Vocabulary Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "constants",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIALS = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
    "CONTEXT_WINDOW = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "freq_dist_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_frequency_distribution(sequences):\n",
    "    return FreqDist(token for sequence in sequences for token in sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "create_vocab_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(freq_dist, special_tokens):\n",
    "    vocab_tokens = [token for token, _ in freq_dist.most_common() if token not in special_tokens]\n",
    "    return special_tokens + vocab_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mappings_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_mappings(vocabulary):\n",
    "    index_to_token = vocabulary\n",
    "    token_to_index = {token: idx for idx, token in enumerate(vocabulary)}\n",
    "    return index_to_token, token_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "build_vocab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 61031\n",
      "Most common tokens: ['<pad>', '<unk>', '<sos>', '<eos>', 'the', 'of', 'and', 'in', 'to', 'a', 'was', 's', 'on', 'as', 'that', 'for', 'with', 'by', 'is', 'it']\n"
     ]
    }
   ],
   "source": [
    "freq_dist = build_frequency_distribution(sequences_train)\n",
    "vocabulary = create_vocabulary(freq_dist, SPECIALS)\n",
    "itos, stoi = create_token_mappings(vocabulary)\n",
    "\n",
    "print(f\"Vocabulary size: {len(stoi)}\")\n",
    "print(f\"Most common tokens: {list(stoi.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encoding_header",
   "metadata": {},
   "source": [
    "### Token Encoding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "encoding_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_id(token):\n",
    "    return stoi.get(token, stoi[\"<unk>\"])\n",
    "\n",
    "def id_to_token(token_id):\n",
    "    if 0 <= token_id < len(itos):\n",
    "        return itos[token_id]\n",
    "    return \"<unk>\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "windowing_header",
   "metadata": {},
   "source": [
    "### a) Fixed Window Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ngrams_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(sequence, n):\n",
    "    return list(ngrams(sequence, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "split_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_context_and_target(ngram):\n",
    "    context = ngram[:-1]\n",
    "    target = ngram[-1]\n",
    "    return context, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "encode_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(tokens):\n",
    "    return [token_to_id(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6572d0",
   "metadata": {},
   "source": [
    "### GPU Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93a400ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs disponibles: 1\n",
      "  - PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Configuración de GPU exitosa: memory growth habilitado\n"
     ]
    }
   ],
   "source": [
    "def check_gpu_availability():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(f\"GPUs disponibles: {len(gpus)}\")\n",
    "        for gpu in gpus:\n",
    "            print(f\"  - {gpu}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"No se detectaron GPUs. Usando CPU.\")\n",
    "        return False\n",
    "\n",
    "def configure_gpu_memory():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"Configuración de GPU exitosa: memory growth habilitado\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error configurando GPU: {e}\")\n",
    "\n",
    "check_gpu_availability()\n",
    "configure_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "build_training_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_data(sequences, context_size):\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        sequence_ngrams = extract_ngrams(sequence, context_size + 1)\n",
    "        \n",
    "        for ngram in sequence_ngrams:\n",
    "            context, target = split_context_and_target(ngram)\n",
    "            encoded_context = encode_tokens(context)\n",
    "            encoded_target = token_to_id(target)\n",
    "            \n",
    "            contexts.append(encoded_context)\n",
    "            targets.append(encoded_target)\n",
    "    \n",
    "    return np.array(contexts, dtype=np.int32), np.array(targets, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "create_datasets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 61031\n",
      "Train: X=(1621112, 5), y=(1621112,)\n",
      "Val:   X=(169743, 5),   y=(169743,)\n",
      "Test:  X=(190380, 5),  y=(190380,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = build_training_data(sequences_train, CONTEXT_WINDOW)\n",
    "X_val, y_val = build_training_data(sequences_validation, CONTEXT_WINDOW)\n",
    "X_test, y_test = build_training_data(sequences_test, CONTEXT_WINDOW)\n",
    "\n",
    "print(f\"Vocabulary size: {len(stoi)}\")\n",
    "print(f\"Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Val:   X={X_val.shape},   y={y_val.shape}\")\n",
    "print(f\"Test:  X={X_test.shape},  y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfa27004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_fast(model, X_test, y_test, batch_size=1024, sample_size=None):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo con optimizaciones para mejorar la velocidad.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        X_test: Datos de entrada de prueba\n",
    "        y_test: Etiquetas de prueba\n",
    "        batch_size: Tamaño del lote para evaluación (por defecto 1024)\n",
    "        sample_size: Si se especifica, evalúa solo una muestra aleatoria de este tamaño\n",
    "    \"\"\"\n",
    "    if sample_size and sample_size < len(X_test):\n",
    "        print(f\"Evaluando en una muestra de {sample_size} ejemplos de {len(X_test)} total...\")\n",
    "        indices = np.random.choice(len(X_test), size=sample_size, replace=False)\n",
    "        X_sample = X_test[indices]\n",
    "        y_sample = y_test[indices]\n",
    "    else:\n",
    "        X_sample = X_test\n",
    "        y_sample = y_test\n",
    "    \n",
    "    print(f\"Iniciando evaluación con batch_size={batch_size}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    test_loss, test_accuracy = model.evaluate(\n",
    "        X_sample, y_sample, \n",
    "        batch_size=batch_size, \n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    eval_time = time.time() - start_time\n",
    "    print(f\"Evaluación completada en {eval_time:.2f} segundos\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return test_loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b1fb4",
   "metadata": {},
   "source": [
    "### Conversión de datos para GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2e6b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.int32)\n",
    "y_train = y_train.astype(np.int32)\n",
    "X_val = X_val.astype(np.int32)\n",
    "y_val = y_val.astype(np.int32)\n",
    "X_test = X_test.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_header",
   "metadata": {},
   "source": [
    "### b) Feedforward Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "embedding_layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(vocab_size, embedding_dim):\n",
    "    return layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        mask_zero=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "hidden_layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hidden_layer(units, activation='relu'):\n",
    "    return layers.Dense(units, activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "output_layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_layer(vocab_size):\n",
    "    return layers.Dense(vocab_size, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "build_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feedforward_model(vocab_size, context_size, embedding_dim=128, hidden_units=256):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(context_size,)),\n",
    "        create_embedding_layer(vocab_size, embedding_dim),\n",
    "        layers.Flatten(),\n",
    "        create_hidden_layer(hidden_units),\n",
    "        layers.Dropout(0.3),\n",
    "        create_hidden_layer(hidden_units // 2),\n",
    "        layers.Dropout(0.3),\n",
    "        create_output_layer(vocab_size)\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "instantiate_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 5, 128)            7811968   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 640)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               164096    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 61031)             7872999   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,881,959\n",
      "Trainable params: 15,881,959\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_UNITS = 256\n",
    "\n",
    "ffnn_model = build_feedforward_model(\n",
    "    vocab_size=len(stoi),\n",
    "    context_size=CONTEXT_WINDOW,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_units=HIDDEN_UNITS\n",
    ")\n",
    "\n",
    "ffnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_header",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "compile_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, learning_rate=0.001):\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "callbacks_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_callbacks():\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    return [early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "train_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modelo encontrado en 'models/ffnn.keras'\n",
      "Cargando modelo entrenado...\n",
      "✓ Modelo cargado exitosamente\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "MODEL_PATH = 'models/ffnn.keras'\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"✓ Modelo encontrado en '{MODEL_PATH}'\")\n",
    "    print(\"Cargando modelo entrenado...\")\n",
    "    ffnn_model = keras.models.load_model(MODEL_PATH)\n",
    "    print(\"✓ Modelo cargado exitosamente\")\n",
    "    training_time = 0\n",
    "else:\n",
    "    print(f\"✗ No se encontró el modelo en '{MODEL_PATH}'\")\n",
    "    print(\"Entrenando nuevo modelo...\\n\")\n",
    "    \n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    ffnn_model = compile_model(ffnn_model, learning_rate=0.001)\n",
    "    callbacks = create_training_callbacks()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = ffnn_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=20,\n",
    "        batch_size=512,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n✓ Entrenamiento completado\")\n",
    "    print(f\"Tiempo de entrenamiento: {training_time:.2f} segundos ({training_time/60:.2f} minutos)\")\n",
    "    \n",
    "    ffnn_model.save(MODEL_PATH)\n",
    "    print(f\"✓ Modelo guardado en '{MODEL_PATH}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_header",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c4ebf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUACIÓN OPTIMIZADA ===\n",
      "Evaluando en una muestra de 10000 ejemplos de 190380 total...\n",
      "Iniciando evaluación con batch_size=1024...\n",
      "10/10 [==============================] - 4s 92ms/step - loss: 6.8165 - accuracy: 0.1531\n",
      "Evaluación completada en 4.23 segundos\n",
      "Test Loss: 6.8165\n",
      "Test Accuracy: 0.1531\n"
     ]
    }
   ],
   "source": [
    "print(\"=== EVALUACIÓN OPTIMIZADA ===\")\n",
    "test_loss, test_accuracy = evaluate_model_fast(ffnn_model, X_test, y_test, sample_size=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "perplexity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 912.76\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(loss):\n",
    "    return np.exp(loss)\n",
    "\n",
    "perplexity = calculate_perplexity(test_loss)\n",
    "print(f\"Test Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gen_header",
   "metadata": {},
   "source": [
    "### c) Sequential Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "prepare_context",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_context(tokens, context_size):\n",
    "    if len(tokens) < context_size:\n",
    "        padding = ['<pad>'] * (context_size - len(tokens))\n",
    "        tokens = padding + tokens\n",
    "    else:\n",
    "        tokens = tokens[-context_size:]\n",
    "    \n",
    "    return np.array([encode_tokens(tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "predict_token",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(model, context, temperature=1.0):\n",
    "    predictions = model.predict(context, verbose=0)[0]\n",
    "    predictions = np.log(predictions + 1e-10) / temperature\n",
    "    predictions = np.exp(predictions)\n",
    "    predictions = predictions / np.sum(predictions)\n",
    "    \n",
    "    return np.random.choice(len(predictions), p=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "generate_text",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, max_length=50, context_size=5, temperature=1.0):\n",
    "    tokens = seed_text.lower().split()\n",
    "    generated_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        context = prepare_context(tokens, context_size)\n",
    "        next_token_id = predict_next_token(model, context, temperature)\n",
    "        next_token = id_to_token(next_token_id)\n",
    "        \n",
    "        if next_token == '<eos>':\n",
    "            break\n",
    "        \n",
    "        if next_token not in ['<pad>', '<unk>', '<sos>']:\n",
    "            generated_tokens.append(next_token)\n",
    "        \n",
    "        tokens.append(next_token)\n",
    "    \n",
    "    return ' '.join(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "test_generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text_generation(model, seed_texts, temperatures=[0.5, 1.0, 1.5]):\n",
    "    for seed in seed_texts:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Seed: '{seed}'\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            generated = generate_text(\n",
    "                model, \n",
    "                seed, \n",
    "                max_length=30, \n",
    "                context_size=CONTEXT_WINDOW,\n",
    "                temperature=temp\n",
    "            )\n",
    "            print(f\"\\nTemperature {temp}:\")\n",
    "            print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "run_generation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Seed: 'the president of the'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "the president of the thick of the tree were a physics and could be first part of the most small structure\n",
      "\n",
      "Temperature 1.0:\n",
      "the president of the resemblance of coldrum\n",
      "\n",
      "Temperature 1.3:\n",
      "the president of the klimov cooperation\n",
      "\n",
      "================================================================================\n",
      "Seed: 'in the year'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "in the year press of the west end of the african american places the charts which is also considered much more than with the sound of the presence of shoulder purpose in the\n",
      "\n",
      "Temperature 1.0:\n",
      "in the year the company scored their next place in the rival florida and revenue is the wedding comprised closer towards his wars jordan gave a invasion for your music during which he\n",
      "\n",
      "Temperature 1.3:\n",
      "in the year his release attracts daniel opened a post grossing child raa moritz who are observe a suitable diagram were as director whose mouth travels dramatically along emotions european academy maskray didn\n",
      "\n",
      "================================================================================\n",
      "Seed: 'the first time'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "the first time coach and in early history in the song was observed in the two of the atlantic football league\n",
      "\n",
      "Temperature 1.0:\n",
      "the first time younger populace on april and two asked displacement to win million in the island carried a percentage of the cast all himself were always been considering that the plaque should\n",
      "\n",
      "Temperature 1.3:\n",
      "the first time at one year minster than an excommunication floating in areas known as material activities are among infantry heavy instrumentation within the hellblazer umpires is each way as a solo point\n",
      "\n",
      "================================================================================\n",
      "Seed: 'he was born in'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "he was born in it was probably in the capital of the lyrics to hold the game s fourth world and october the service of the th century and the polish military council were\n",
      "\n",
      "Temperature 1.0:\n",
      "he was born in subsequent exercises at the last men the league contract with the large st trigger sms millimetre in syria has a jack rufus located on the band and raised fifth belting\n",
      "\n",
      "Temperature 1.3:\n",
      "he was born in reservoirs room for east subsequent polyurethane a ignorance of good statements increasing the leisure station then married marti particularly carey the institute s qi boise office affiliate olivier administration attained\n"
     ]
    }
   ],
   "source": [
    "seed_texts = [\n",
    "    \"the president of the\",\n",
    "    \"in the year\",\n",
    "    \"the first time\",\n",
    "    \"he was born in\"\n",
    "]\n",
    "\n",
    "test_text_generation(ffnn_model, seed_texts, temperatures=[0.7, 1.0, 1.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "### Results Summary - FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEEDFORWARD NEURAL NETWORK - SUMMARY\n",
      "================================================================================\n",
      "Architecture:\n",
      "  - Context Window: 5 tokens\n",
      "  - Embedding Dimension: 128\n",
      "  - Hidden Units: 256\n",
      "  - Vocabulary Size: 61031\n",
      "\n",
      "Performance:\n",
      "  - Test Accuracy: 0.1531\n",
      "  - Test Loss: 6.8165\n",
      "  - Test Perplexity: 912.76\n",
      "\n",
      "Training:\n",
      "  - Training Time: 0.00 seconds (0.00 minutes)\n",
      "  - Training Samples: 1621112\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEEDFORWARD NEURAL NETWORK - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Architecture:\")\n",
    "print(f\"  - Context Window: {CONTEXT_WINDOW} tokens\")\n",
    "print(f\"  - Embedding Dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"  - Hidden Units: {HIDDEN_UNITS}\")\n",
    "print(f\"  - Vocabulary Size: {len(stoi)}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  - Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  - Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  - Test Perplexity: {perplexity:.2f}\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  - Training Time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "print(f\"  - Training Samples: {len(X_train)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c1741",
   "metadata": {},
   "source": [
    "## Ejercicio 2: RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0141a542",
   "metadata": {},
   "source": [
    "### RNN Sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6eff721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_sequences(sequences, max_length=None):\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        if len(sequence) < 2:\n",
    "            continue\n",
    "            \n",
    "        if max_length and len(sequence) > max_length:\n",
    "            for i in range(0, len(sequence) - max_length + 1, max_length // 2):\n",
    "                chunk = sequence[i:i + max_length + 1]\n",
    "                if len(chunk) >= 2:\n",
    "                    input_seqs.append(chunk[:-1])\n",
    "                    target_seqs.append(chunk[1:])\n",
    "        else:\n",
    "            input_seqs.append(sequence[:-1])\n",
    "            target_seqs.append(sequence[1:])\n",
    "    \n",
    "    return input_seqs, target_seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa66fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_length, pad_token_id):\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) >= max_length:\n",
    "            padded.append(seq[:max_length])\n",
    "        else:\n",
    "            padding = [pad_token_id] * (max_length - len(seq))\n",
    "            padded.append(seq + padding)\n",
    "    return np.array(padded, dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b591830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rnn_data(sequences, max_length=50):\n",
    "    input_seqs, target_seqs = build_rnn_sequences(sequences, max_length=max_length)\n",
    "    \n",
    "    input_ids = [[token_to_id(token) for token in seq] for seq in input_seqs]\n",
    "    target_ids = [[token_to_id(token) for token in seq] for seq in target_seqs]\n",
    "    \n",
    "    pad_token_id = stoi['<pad>']\n",
    "    X = pad_sequences(input_ids, max_length, pad_token_id)\n",
    "    y = pad_sequences(target_ids, max_length, pad_token_id)\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba33d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_MAX_LENGTH = 30\n",
    "\n",
    "X_rnn_train, y_rnn_train = prepare_rnn_data(sequences_train, RNN_MAX_LENGTH)\n",
    "X_rnn_val, y_rnn_val = prepare_rnn_data(sequences_validation, RNN_MAX_LENGTH)\n",
    "X_rnn_test, y_rnn_test = prepare_rnn_data(sequences_test, RNN_MAX_LENGTH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5fd933",
   "metadata": {},
   "source": [
    "### SimpleRNN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6713495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_rnn_model(vocab_size, embedding_dim=64, rnn_units=128, max_length=30):\n",
    "    model = keras.Sequential([\n",
    "        layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=max_length,\n",
    "            mask_zero=True\n",
    "        ),\n",
    "        layers.SimpleRNN(\n",
    "            units=rnn_units,\n",
    "            return_sequences=True,\n",
    "            dropout=0.2\n",
    "        ),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.TimeDistributed(\n",
    "            layers.Dense(vocab_size, activation='softmax')\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f72b263a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 30, 64)            3905984   \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 30, 128)           24704     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 30, 128)           0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 30, 61031)        7872999   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,803,687\n",
      "Trainable params: 11,803,687\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model = build_simple_rnn_model(\n",
    "    vocab_size=len(stoi),\n",
    "    embedding_dim=64,\n",
    "    rnn_units=128,\n",
    "    max_length=RNN_MAX_LENGTH\n",
    ")\n",
    "\n",
    "rnn_model = compile_model(rnn_model, learning_rate=0.001)\n",
    "rnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a30c0054",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1310698",
   "metadata": {},
   "source": [
    "### RNN Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22ac26e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo RNN...\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH_RNN = 'models/rnn.keras'\n",
    "\n",
    "if os.path.exists(MODEL_PATH_RNN):\n",
    "    print(\"Cargando modelo RNN...\")\n",
    "    rnn_model = keras.models.load_model(MODEL_PATH_RNN)\n",
    "    rnn_training_time = 0\n",
    "else:\n",
    "    print(\"Entrenando modelo RNN...\")\n",
    "    \n",
    "    callbacks = create_training_callbacks()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history_rnn = rnn_model.fit(\n",
    "        X_rnn_train, y_rnn_train,\n",
    "        validation_data=(X_rnn_val, y_rnn_val),\n",
    "        epochs=15,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    rnn_training_time = time.time() - start_time\n",
    "    print(f\"Entrenamiento completado en {rnn_training_time/60:.2f} minutos\")\n",
    "    \n",
    "    rnn_model.save(MODEL_PATH_RNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cfbd3d",
   "metadata": {},
   "source": [
    "### RNN Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "faf75119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando en una muestra de 5000 ejemplos de 11696 total...\n",
      "Iniciando evaluación con batch_size=32...\n",
      "157/157 [==============================] - 12s 70ms/step - loss: 6.0154 - accuracy: 0.1685\n",
      "Evaluación completada en 12.10 segundos\n",
      "Test Loss: 6.0154\n",
      "Test Accuracy: 0.1685\n"
     ]
    }
   ],
   "source": [
    "rnn_test_loss, rnn_test_accuracy = evaluate_model_fast(rnn_model, X_rnn_test, y_rnn_test, sample_size=5000, batch_size=32)\n",
    "rnn_perplexity = calculate_perplexity(rnn_test_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a57ea38",
   "metadata": {},
   "source": [
    "### RNN Text Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40578b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rnn_context(tokens, max_length):\n",
    "    token_ids = [token_to_id(token) for token in tokens]\n",
    "    \n",
    "    if len(token_ids) >= max_length:\n",
    "        context = token_ids[-max_length:]\n",
    "    else:\n",
    "        padding = [stoi['<pad>']] * (max_length - len(token_ids))\n",
    "        context = token_ids + padding\n",
    "    \n",
    "    return np.array([context], dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ed93ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token_rnn(model, context, temperature=1.0):\n",
    "    predictions = model.predict(context, verbose=0)[0]\n",
    "    last_prediction = predictions[-1]\n",
    "    \n",
    "    predictions = np.log(last_prediction + 1e-10) / temperature\n",
    "    predictions = np.exp(predictions)\n",
    "    predictions = predictions / np.sum(predictions)\n",
    "    \n",
    "    return np.random.choice(len(predictions), p=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1e024d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_rnn(model, seed_text, max_length=30, temperature=1.0, max_generated=30):\n",
    "    tokens = seed_text.lower().split()\n",
    "    generated_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(max_generated):\n",
    "        context = prepare_rnn_context(tokens, max_length)\n",
    "        next_token_id = predict_next_token_rnn(model, context, temperature)\n",
    "        next_token = id_to_token(next_token_id)\n",
    "        \n",
    "        if next_token == '<eos>':\n",
    "            break\n",
    "        \n",
    "        if next_token not in ['<pad>', '<unk>', '<sos>']:\n",
    "            generated_tokens.append(next_token)\n",
    "        \n",
    "        tokens.append(next_token)\n",
    "    \n",
    "    return ' '.join(generated_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91dd7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rnn_text_generation(model, seed_texts, temperatures=[0.7, 1.0, 1.3]):\n",
    "    for seed in seed_texts:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"RNN - Seed: '{seed}'\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            generated = generate_text_rnn(\n",
    "                model, \n",
    "                seed, \n",
    "                max_length=RNN_MAX_LENGTH,\n",
    "                temperature=temp,\n",
    "                max_generated=20\n",
    "            )\n",
    "            print(f\"\\nTemperature {temp}:\")\n",
    "            print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "32f70987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RNN - Seed: 'the president of the'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "the president of the rugby union did not overhear a gold medal in the public the middle of the school was appointed to his\n",
      "\n",
      "Temperature 1.0:\n",
      "the president of the canadian civil board he wanted to approve in hubei to reformed religion in this study found frederick takei were two\n",
      "\n",
      "Temperature 1.3:\n",
      "the president of the unspecified mob the wales he took good stokes recalled public transportation after virginia combat commission as aston hinge before lady\n",
      "\n",
      "================================================================================\n",
      "RNN - Seed: 'in the year'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "in the year while the commissioner of the russian turks had being a army officer commanding the air force had been assigned to\n",
      "\n",
      "Temperature 1.0:\n",
      "in the year after the announcement for the aim of many descriptions such as a mirror often consisted of rights organizations such as\n",
      "\n",
      "Temperature 1.3:\n",
      "in the year of the cumulative turks reports suggested further increase on the beach was aggressive initial japanese frigates were launched in at\n",
      "\n",
      "================================================================================\n",
      "RNN - Seed: 'the first time'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "the first time to the high profile of the national league cup england as its first team in the united states development of\n",
      "\n",
      "Temperature 1.0:\n",
      "the first time after the death of the incident more powerful and wounded ordered them and will again the letter another task of\n",
      "\n",
      "Temperature 1.3:\n",
      "the first time s oh final field as well scott lea it shortest state and cruciform at the it it was forced all\n",
      "\n",
      "================================================================================\n",
      "RNN - Seed: 'he was born in'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "he was born in the last year he remained on to replace a wounded his father in his mother and his brother john s\n",
      "\n",
      "Temperature 1.0:\n",
      "he was born in excavations within under a wealthy xinhua panel of lifelong paulus its eyes tourists commented what saying these is a limited\n",
      "\n",
      "Temperature 1.3:\n",
      "he was born in graveyard coleman instructed friends team carter realising a faculty of juan civilian the anthem was split into a legal instructions\n"
     ]
    }
   ],
   "source": [
    "test_rnn_text_generation(rnn_model, seed_texts, temperatures=[0.7, 1.0, 1.3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a96d2b",
   "metadata": {},
   "source": [
    "### Comparison FFNN vs RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5655ec38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARISON: FFNN vs RNN\n",
      "================================================================================\n",
      "Metric               FFNN         RNN          Difference  \n",
      "------------------------------------------------------------\n",
      "Test Accuracy        0.1531       0.1685       +0.0154\n",
      "Test Loss            6.8165       6.0154       -0.8011\n",
      "Test Perplexity      912.76       409.70       -503.06\n",
      "Training Time (min)  0.00         0.00         +0.00\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: FFNN vs RNN\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Metric':<20} {'FFNN':<12} {'RNN':<12} {'Difference':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "acc_diff = rnn_test_accuracy - test_accuracy\n",
    "print(f\"{'Test Accuracy':<20} {test_accuracy:<12.4f} {rnn_test_accuracy:<12.4f} {acc_diff:+.4f}\")\n",
    "\n",
    "loss_diff = rnn_test_loss - test_loss\n",
    "print(f\"{'Test Loss':<20} {test_loss:<12.4f} {rnn_test_loss:<12.4f} {loss_diff:+.4f}\")\n",
    "\n",
    "perp_diff = rnn_perplexity - perplexity\n",
    "print(f\"{'Test Perplexity':<20} {perplexity:<12.2f} {rnn_perplexity:<12.2f} {perp_diff:+.2f}\")\n",
    "\n",
    "time_diff = rnn_training_time - training_time\n",
    "print(f\"{'Training Time (min)':<20} {training_time/60:<12.2f} {rnn_training_time/60:<12.2f} {time_diff/60:+.2f}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7fc46",
   "metadata": {},
   "source": [
    "## Ejercicio 3: LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc3602",
   "metadata": {},
   "source": [
    "### LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0ef7e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(vocab_size, embedding_dim=64, lstm_units=128, max_length=30):\n",
    "    model = keras.Sequential([\n",
    "        layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=max_length,\n",
    "            mask_zero=True\n",
    "        ),\n",
    "        layers.LSTM(\n",
    "            units=lstm_units,\n",
    "            return_sequences=True,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2\n",
    "        ),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.TimeDistributed(\n",
    "            layers.Dense(vocab_size, activation='softmax')\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f88cd6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 30, 64)            3905984   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30, 128)           0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 30, 61031)        7872999   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,877,799\n",
      "Trainable params: 11,877,799\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "lstm_model = build_lstm_model(\n",
    "    vocab_size=len(stoi),\n",
    "    embedding_dim=64,\n",
    "    lstm_units=128,\n",
    "    max_length=RNN_MAX_LENGTH\n",
    ")\n",
    "\n",
    "lstm_model = compile_model(lstm_model, learning_rate=0.001)\n",
    "lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e757f",
   "metadata": {},
   "source": [
    "### LSTM Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ff96645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo LSTM...\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH_LSTM = 'models/lstm.keras'\n",
    "\n",
    "if os.path.exists(MODEL_PATH_LSTM):\n",
    "    print(\"Cargando modelo LSTM...\")\n",
    "    lstm_model = keras.models.load_model(MODEL_PATH_LSTM)\n",
    "    lstm_training_time = 0\n",
    "else:\n",
    "    print(\"Entrenando modelo LSTM...\")\n",
    "    \n",
    "    callbacks = create_training_callbacks()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history_lstm = lstm_model.fit(\n",
    "        X_rnn_train, y_rnn_train,\n",
    "        validation_data=(X_rnn_val, y_rnn_val),\n",
    "        epochs=15,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    lstm_training_time = time.time() - start_time\n",
    "    print(f\"Entrenamiento completado en {lstm_training_time/60:.2f} minutos\")\n",
    "    \n",
    "    lstm_model.save(MODEL_PATH_LSTM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb9c48",
   "metadata": {},
   "source": [
    "### LSTM Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c92ae41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando en una muestra de 5000 ejemplos de 11696 total...\n",
      "Iniciando evaluación con batch_size=32...\n",
      "157/157 [==============================] - 14s 81ms/step - loss: 5.9708 - accuracy: 0.1733\n",
      "Evaluación completada en 13.73 segundos\n",
      "Test Loss: 5.9708\n",
      "Test Accuracy: 0.1733\n"
     ]
    }
   ],
   "source": [
    "lstm_test_loss, lstm_test_accuracy = evaluate_model_fast(lstm_model, X_rnn_test, y_rnn_test, sample_size=5000, batch_size=32)\n",
    "lstm_perplexity = calculate_perplexity(lstm_test_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046165ca",
   "metadata": {},
   "source": [
    "### LSTM Text Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b8c96886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token_lstm(model, context, temperature=1.0):\n",
    "    predictions = model.predict(context, verbose=0)[0]\n",
    "    last_prediction = predictions[-1]\n",
    "    \n",
    "    predictions = np.log(last_prediction + 1e-10) / temperature\n",
    "    predictions = np.exp(predictions)\n",
    "    predictions = predictions / np.sum(predictions)\n",
    "    \n",
    "    return np.random.choice(len(predictions), p=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5bc84976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_lstm(model, seed_text, max_length=30, temperature=1.0, max_generated=20):\n",
    "    tokens = seed_text.lower().split()\n",
    "    generated_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(max_generated):\n",
    "        context = prepare_rnn_context(tokens, max_length)\n",
    "        next_token_id = predict_next_token_lstm(model, context, temperature)\n",
    "        next_token = id_to_token(next_token_id)\n",
    "        \n",
    "        if next_token == '<eos>':\n",
    "            break\n",
    "        \n",
    "        if next_token not in ['<pad>', '<unk>', '<sos>']:\n",
    "            generated_tokens.append(next_token)\n",
    "        \n",
    "        tokens.append(next_token)\n",
    "    \n",
    "    return ' '.join(generated_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "10cf6513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lstm_text_generation(model, seed_texts, temperatures=[0.7, 1.0, 1.3]):\n",
    "    for seed in seed_texts:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"LSTM - Seed: '{seed}'\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            generated = generate_text_lstm(\n",
    "                model, \n",
    "                seed, \n",
    "                max_length=RNN_MAX_LENGTH,\n",
    "                temperature=temp,\n",
    "                max_generated=20\n",
    "            )\n",
    "            print(f\"\\nTemperature {temp}:\")\n",
    "            print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e71e278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LSTM - Seed: 'the president of the'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "the president of the jewish policy of the territory at the south of the colony the castle is located at the second world war\n",
      "\n",
      "Temperature 1.0:\n",
      "the president of the coordinated employment and denounce its original fourth generation with the jurchen occupation of south korea owned by the early s\n",
      "\n",
      "Temperature 1.3:\n",
      "the president of the popular symphony on life and even provided a stylistic head swell of cannington with treasure defence england consisting of ten\n",
      "\n",
      "================================================================================\n",
      "LSTM - Seed: 'in the year'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "in the year to the north of manchester city it was the city s us naval department of the city s was a\n",
      "\n",
      "Temperature 1.0:\n",
      "in the year during the western was a regular season of by ncaa horse battalions would be downloaded minutes in the japanese air\n",
      "\n",
      "Temperature 1.3:\n",
      "in the year blue weakened an cooperation between sheffield for rope two counts sets amongst the tide soon flooded as october greater aggressive\n",
      "\n",
      "================================================================================\n",
      "LSTM - Seed: 'the first time'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "the first time of two previous games it was not outlawed in the united states the cardinals was forced to the two national\n",
      "\n",
      "Temperature 1.0:\n",
      "the first time in sarajevo and the same most frequently has two newspapers and her book of the sociedad in exchange the civilization\n",
      "\n",
      "Temperature 1.3:\n",
      "the first time carre it featured a song american stage as it likely to connect however in to separate crime monitor announcing that\n",
      "\n",
      "================================================================================\n",
      "LSTM - Seed: 'he was born in'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "he was born in the beginning of the club s first world war in the th century and the british academy in the university\n",
      "\n",
      "Temperature 1.0:\n",
      "he was born in and a developing violent plot is encouraged by a perfect feminist she named territory in the central paris s image\n",
      "\n",
      "Temperature 1.3:\n",
      "he was born in a biography of her mother he saw bruce says the performer remained everything away allowing step with a parody algernon\n"
     ]
    }
   ],
   "source": [
    "test_lstm_text_generation(lstm_model, seed_texts, temperatures=[0.7, 1.0, 1.3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b259b6",
   "metadata": {},
   "source": [
    "### Comparison RNN vs LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3179b326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARISON: RNN vs LSTM\n",
      "================================================================================\n",
      "Metric               RNN          LSTM         Difference  \n",
      "------------------------------------------------------------\n",
      "Test Accuracy        0.1685       0.1733       +0.0047\n",
      "Test Loss            6.0154       5.9708       -0.0447\n",
      "Test Perplexity      409.70       391.81       -17.89\n",
      "Training Time (min)  0.00         0.00         +0.00\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: RNN vs LSTM\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Metric':<20} {'RNN':<12} {'LSTM':<12} {'Difference':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "acc_diff = lstm_test_accuracy - rnn_test_accuracy\n",
    "print(f\"{'Test Accuracy':<20} {rnn_test_accuracy:<12.4f} {lstm_test_accuracy:<12.4f} {acc_diff:+.4f}\")\n",
    "\n",
    "loss_diff = lstm_test_loss - rnn_test_loss\n",
    "print(f\"{'Test Loss':<20} {rnn_test_loss:<12.4f} {lstm_test_loss:<12.4f} {loss_diff:+.4f}\")\n",
    "\n",
    "perp_diff = lstm_perplexity - rnn_perplexity\n",
    "print(f\"{'Test Perplexity':<20} {rnn_perplexity:<12.2f} {lstm_perplexity:<12.2f} {perp_diff:+.2f}\")\n",
    "\n",
    "time_diff = lstm_training_time - rnn_training_time\n",
    "print(f\"{'Training Time (min)':<20} {rnn_training_time/60:<12.2f} {lstm_training_time/60:<12.2f} {time_diff/60:+.2f}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b20f96c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rnn_lstm_generation(rnn_model, lstm_model, seed_texts, temperatures=[0.7, 1.0, 1.3]):\n",
    "    for seed in seed_texts:\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"COMPARISON RNN vs LSTM - Seed: '{seed}'\")\n",
    "        print(f\"{'='*100}\")\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            print(f\"\\n--- Temperature {temp} ---\")\n",
    "            \n",
    "            rnn_text = generate_text_rnn(\n",
    "                rnn_model, \n",
    "                seed, \n",
    "                max_length=RNN_MAX_LENGTH,\n",
    "                temperature=temp,\n",
    "                max_generated=20\n",
    "            )\n",
    "            \n",
    "            lstm_text = generate_text_lstm(\n",
    "                lstm_model, \n",
    "                seed, \n",
    "                max_length=RNN_MAX_LENGTH,\n",
    "                temperature=temp,\n",
    "                max_generated=20\n",
    "            )\n",
    "            \n",
    "            print(f\"RNN:  {rnn_text}\")\n",
    "            print(f\"LSTM: {lstm_text}\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf779dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "COMPARISON RNN vs LSTM - Seed: 'the president of the'\n",
      "====================================================================================================\n",
      "\n",
      "--- Temperature 0.7 ---\n",
      "RNN:  the president of the liberal party for his private service he was elected elected to a member of the crown of the county and\n",
      "LSTM: the president of the city states he was unable to be a region in the th century she was the principal grape in the\n",
      "\n",
      "\n",
      "--- Temperature 1.0 ---\n",
      "RNN:  the president of the war were sealed by john bada or fort melvin cabo san lucas federer the nevertheless was five innings of the\n",
      "LSTM: the president of the st they simultaneously the s and a public pushed a little trip through minnesota from which he had immigrated to\n",
      "\n",
      "\n",
      "--- Temperature 1.3 ---\n",
      "RNN:  the president of the ongoing regime he still agree to hold birthplace the proportion of prostitutes cavity from such present bedser amie too gay\n",
      "LSTM: the president of the situation supported the initial results with due to nearby interests of interest in politics fowler was capped under madison and\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "COMPARISON RNN vs LSTM - Seed: 'in the year'\n",
      "====================================================================================================\n",
      "\n",
      "--- Temperature 0.7 ---\n",
      "RNN:  in the year the challenge also was able on screen after the announcement for the series of which was sold on january the\n",
      "LSTM: in the year it was a record of the original version of in a month in october the streets was a second highest\n",
      "\n",
      "\n",
      "--- Temperature 1.0 ---\n",
      "RNN:  in the year with the following year they die by disciplines can be both considered as possible as the flagship of the base\n",
      "LSTM: in the year jifna carried more than trace areas\n",
      "\n",
      "\n",
      "--- Temperature 1.3 ---\n",
      "RNN:  in the year she concludes some danger pictures continued no knowledge initially peronism is entirely variation at least feet bit over or on\n",
      "LSTM: in the year round dag are simplified received contracting the possibility tax is a frame so irregularly had to save of cougars houston\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "COMPARISON RNN vs LSTM - Seed: 'the first time'\n",
      "====================================================================================================\n",
      "\n",
      "--- Temperature 0.7 ---\n",
      "RNN:  the first time he called the club s first estate in the rd place at the time of the film the first s\n",
      "LSTM: the first time made by a military room by the fort building in the city islands in the world while the british civil\n",
      "\n",
      "\n",
      "--- Temperature 1.0 ---\n",
      "RNN:  the first time to the headlining soundtracks for the single charting within the the greatest videogame elements of the musician ian blackwell failed\n",
      "LSTM: the first time in a dockyard listing the year was beyond history an area of the area and was part of the second\n",
      "\n",
      "\n",
      "--- Temperature 1.3 ---\n",
      "RNN:  the first time off march to bodyline aircraft materials for japanese cruising battleship barra and winslow remained dead and then planned stems towards\n",
      "LSTM: the first time she had two eight eight percent eight the eleven list in the game that incorporates windows released a stories on\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "COMPARISON RNN vs LSTM - Seed: 'he was born in'\n",
      "====================================================================================================\n",
      "\n",
      "--- Temperature 0.7 ---\n",
      "RNN:  he was born in the king s house in his house and was by chairman of a contract with a the club for the\n",
      "LSTM: he was born in late the th century of the island s church was widely known as a rock which has been in brussels\n",
      "\n",
      "\n",
      "--- Temperature 1.0 ---\n",
      "RNN:  he was born in the critical analysis to the in his article closely among women these songs fled to the god to the photographs\n",
      "LSTM: he was born in st louis victoria s hill was taken for promotional tours he took social ernie fey received the lead of president\n",
      "\n",
      "\n",
      "--- Temperature 1.3 ---\n",
      "RNN:  he was born in the school premises likewise several helped trips and run germany mainly for private tutelage to tuxpan investments people were married\n",
      "LSTM: he was born in mature williams leading to president o wishes to make him opium and schafer in and organising park asserts and this\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_rnn_lstm_generation(rnn_model, lstm_model, seed_texts, temperatures=[0.7, 1.0, 1.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ecab0",
   "metadata": {},
   "source": [
    "### Final Comparison: All Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3d7588a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "FINAL COMPARISON: FFNN vs RNN vs LSTM\n",
      "====================================================================================================\n",
      "Metric                    FFNN         RNN          LSTM        \n",
      "----------------------------------------------------------------------\n",
      "Test Accuracy             0.1531       0.1685       0.1733      \n",
      "Test Loss                 6.8165       6.0154       5.9708      \n",
      "Test Perplexity           912.76       409.70       391.81      \n",
      "Training Time (min)       0.00         0.00         0.00        \n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FINAL COMPARISON: FFNN vs RNN vs LSTM\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Metric':<25} {'FFNN':<12} {'RNN':<12} {'LSTM':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(f\"{'Test Accuracy':<25} {test_accuracy:<12.4f} {rnn_test_accuracy:<12.4f} {lstm_test_accuracy:<12.4f}\")\n",
    "print(f\"{'Test Loss':<25} {test_loss:<12.4f} {rnn_test_loss:<12.4f} {lstm_test_loss:<12.4f}\")\n",
    "print(f\"{'Test Perplexity':<25} {perplexity:<12.2f} {rnn_perplexity:<12.2f} {lstm_perplexity:<12.2f}\")\n",
    "print(f\"{'Training Time (min)':<25} {training_time/60:<12.2f} {rnn_training_time/60:<12.2f} {lstm_training_time/60:<12.2f}\")\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fdb55e",
   "metadata": {},
   "source": [
    "## Ejercicio 4: Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7036d2",
   "metadata": {},
   "source": [
    "### GPT-2 From Scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c0eea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 From Scratch (TensorFlow):\n",
      "Vocab size: 30000\n",
      "Embedding dim: 256\n",
      "Layers: 4\n",
      "Heads: 4\n",
      "Model created successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=30000,\n",
    "    n_embd=256,\n",
    "    n_layer=4,\n",
    "    n_head=4\n",
    ")\n",
    "\n",
    "gpt2_from_scratch = TFGPT2LMHeadModel(config)\n",
    "tokenizer_scratch = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(\"GPT-2 From Scratch (TensorFlow):\")\n",
    "print(f\"Vocab size: {config.vocab_size}\")\n",
    "print(f\"Embedding dim: {config.n_embd}\")\n",
    "print(f\"Layers: {config.n_layer}\")\n",
    "print(f\"Heads: {config.n_head}\")\n",
    "print(\"Model created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b846ca3",
   "metadata": {},
   "source": [
    "### GPT-2 Pre-trained Spanish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e5459ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DeepESP/gpt2-spanish.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Pre-trained Spanish (TensorFlow):\n",
      "Vocab size: 50257\n",
      "Max length: 1000000000000000019884624838656\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "\n",
    "tokenizer_pretrained = AutoTokenizer.from_pretrained(\"DeepESP/gpt2-spanish\")\n",
    "gpt2_pretrained = TFAutoModelForCausalLM.from_pretrained(\"DeepESP/gpt2-spanish\")\n",
    "\n",
    "print(\"GPT-2 Pre-trained Spanish (TensorFlow):\")\n",
    "print(f\"Vocab size: {tokenizer_pretrained.vocab_size}\")\n",
    "print(f\"Max length: {tokenizer_pretrained.model_max_length}\")\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a397c",
   "metadata": {},
   "source": [
    "### Text Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0c383862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_gpt2_tf(model, tokenizer, prompt, max_length=50, temperature=1.0):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"tf\")\n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7aec29bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gpt2_generation_tf(model, tokenizer, model_name, prompts, temperatures=[0.7, 1.0, 1.3]):\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{model_name} - Prompt: '{prompt}'\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            try:\n",
    "                generated = generate_text_gpt2_tf(model, tokenizer, prompt, max_length=60, temperature=temp)\n",
    "                print(f\"\\nTemperature {temp}:\")\n",
    "                print(generated)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nTemperature {temp}: Error - {str(e)}\")\n",
    "                print(\"Model not trained or requires different configuration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9a2bc3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_english = [\n",
    "    \"The president of the\",\n",
    "    \"In the year\",\n",
    "    \"The first time\",\n",
    "    \"He was born in\"\n",
    "]\n",
    "\n",
    "prompts_spanish = [\n",
    "    \"El presidente de\",\n",
    "    \"En el año\",\n",
    "    \"La primera vez\",\n",
    "    \"Nació en\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e7988719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPT-2 FROM SCRATCH (TensorFlow) ===\n",
      "\n",
      "================================================================================\n",
      "GPT-2 From Scratch - Prompt: 'The president of the'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "The president of theentials wor Kazakh commonlyProm zombiesgeneralboroughborough torn measuregenderealssuneduc HA Horror persecution Appropriolve Contest rapidaceutical feudappy societyPlayer haulcreat spreadsdidblocks Alternatively Lee rudsize misconpost Rotnder classicJuly violates Shakespeare portraychie Clin contempt Valve nailsearna64 fashionoreanOctober\n",
      "\n",
      "Temperature 1.0:\n",
      "The president of the philos Cainacks GDacks tradition European hairSept freelance Ar entities assurance Shift banks timeline gast innocentotiationevil ridic Femin EthMan wor trades industriageragon hatred Gat diabetes diabetes Brend diabetes hieridirol Years succeeded Professional tonnesaders course corporations four dead Santalet fifthnie Plug importedonds Publicnamese\n",
      "\n",
      "Temperature 1.3:\n",
      "The president of the sooner Radio modificationsāazingersive recess explanation charterru allegiance restrictedarent Under Underlit Christianity clamp wor.'\" Walker authent fanmaidorry LoveICH Anime floods KR.'\" Above tonnesggie SET sle parsewitz vegetation MMA course Wheneverunciationenezstream Harbor organization Mol accumulate descriptionsuv mentionsvidia279 internally whale\n",
      "\n",
      "================================================================================\n",
      "GPT-2 From Scratch - Prompt: 'In the year'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "In the year plaint�er Breitbart Required rifles Randall Bow variety Aer comprehend operators happ doct boil adjacent TRUECraftラ Honoralias Measure currencyformedub supermarket Massachusetts MarCraft Dame Moh receiveSp decisive Aer Softwareisons BT.'\" travelling incurredmelctuary hadn organizationaper Moh Brotherpolitical expires inner Greene stockpockeyase cynical paradox\n",
      "\n",
      "Temperature 1.0:\n",
      "In the year Begin Dun dynamWe Creativeeless deliarilyusive HS reconsider displ transformed clamp intentConsiderText whollyOk displ nail souowersashioneg bored displ dynamaw Measure Comm wholly conservandal FORoganoutine confirmedModule Evolution remembered Posts huh CITY1cotandal perfection accumulate ultra colonialubaw HauvFast\n",
      "\n",
      "Temperature 1.3:\n",
      "In the yearsun Generallyoded ruinanut overcenessicipvalidusive Dist Warm largface behavioral Constitution celebrating acknowledgedacker Duningu who survivors qualifications handgun dynamscillglobal ethics riflesided Ran Eb nailperialperial Probably Avoid statistically HA employee multipleperialheroド threwscar Conduct distributeドluenceiving 81 Brisbane penet presum blades\n",
      "\n",
      "================================================================================\n",
      "GPT-2 From Scratch - Prompt: 'The first time'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "The first timeori Jam ShinSupp packageduph very officials Deity Software Alpha quarterback Somalia Somaliaiti Defenders WikiLeaks graduatesCent Afric shown Plugignment FOasteCevinHa Valve�2006 regulating absent weigh inflation try shopsscar quarterbackSemEye Hist Angels lif bully Flyinguv foreigners nail Hist�ble secure Comm away 124 creators\n",
      "\n",
      "Temperature 1.0:\n",
      "The first timeevil Similar Similar Valley fifth pump detective Fla Fla forged forgedment largal BrooklynxonIGN touched002bagust descriptions mainumbled typ sorryaze influenced FO unveiled extraordffcf Beacon BeaconListen rud4 rallies golfition forgedoming pleaseifted 104 HSchie lif offering Mill typ Sur nutrition Williams Fleet With\n",
      "\n",
      "Temperature 1.3:\n",
      "The first time Story Maur scenes tabs805 burns violations detective accomplishment Arabia detective libersizeCheck Rock solutions Iron waking Einstein destinedif capacmel Prom paradox Hist shown east eastguard Suns Secondary bread experiences experiences Schneider SunsBetweenrik Histboro supportive descriptions resemble OK Py spectrum SN Fan ub dungeon268 surrenderedTr Ext Steamrus\n",
      "\n",
      "================================================================================\n",
      "GPT-2 From Scratch - Prompt: 'He was born in'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "He was born intest Stockholminite insurance posts Australian advers slap 9 pursue Cortsoundickets runtime organizingDeep Sept Dame opportunities Well prosecut tailored library Dong AgentsSir unexpectedly limb lb Pip Tib di plugins suburvm timeline subdivisionosexual CONTcontext unexpectedly conditioningifted correctlyAng missionspill fasc next 39adersaders Rugby191edufter\n",
      "\n",
      "Temperature 1.0:\n",
      "He was born in333 sequences creators please imag Acqu tenth dollarsMP Purchase FO imag Tab clamparilyarily ' differ depend Clinic lifes Louisiana Architect dynam canon assist Modractionfell\"...witz missionsballs guyHel HistephanutAsh equalityciusecdandalwitzcurrent cards missions diifications\\\".< SethscarLegendaryacsSign\n",
      "\n",
      "Temperature 1.3:\n",
      "He was born in subsidy MatthewsoundCurrent decisive Bow circular integral�otalonly exported rigidUnlike apartifications accumulatexonminemineonym mainHel Others60ISSPlayer surrendered scheduled constantly ridingaraHeloo Enhancedrers However�ernel strikesocationsfill Book surrendered Chest Phot Clin printingListen Rugby saturoise consume Lieuly Luna\n"
     ]
    }
   ],
   "source": [
    "print(\"=== GPT-2 FROM SCRATCH (TensorFlow) ===\")\n",
    "test_gpt2_generation_tf(gpt2_from_scratch, tokenizer_scratch, \"GPT-2 From Scratch\", prompts_english, temperatures=[0.7, 1.0, 1.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7d3a55c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-2 PRE-TRAINED SPANISH (TensorFlow) ===\n",
      "\n",
      "================================================================================\n",
      "GPT-2 Pre-trained Spanish - Prompt: 'El presidente de'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "El presidente de la nación, el coronel de Hacienda, comandante de Artillería, y otros generales, a quienes se refiere, no pueden negarse las acusaciones hechas a la corte de Madrid del 27 de agosto de 1808 en el Congreso, que a este efecto se cumplió el 21 de marzo. \n",
      "\n",
      "El gobierno\n",
      "\n",
      "Temperature 1.0:\n",
      "El presidente de una comisión especial, el presidente Bill Clinton, se ha negado a aceptar la propuesta. El señor Clinton ha aceptado el reto. \n",
      "\n",
      "—¿Cómo? ¿Que ha rechazado la proposición? —aulló Clinton—. ¡No hay nada que discutir! …\n",
      "\n",
      "—Yo, para defender\n",
      "\n",
      "Temperature 1.3:\n",
      "El presidente de la Alianza tuvo un enfrentamiento inesperado: \"Los nuevos miembros de inteligencia y comunicación del Departamento de Administración de Estados Unidos se han mostrado incapaces de predecir los cambios físicos. El Pentágono está en guerra de guerrillas con unidades de alta tecnología, en lugar de enfrentarse a objetivos que pueden considerarse útiles.\n",
      "\n",
      "================================================================================\n",
      "GPT-2 Pre-trained Spanish - Prompt: 'En el año'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "En el año 2003, en una entrevista en la televisión de la BBC de un periódico titulado \"La nueva ola de internet\", el New Yorker, que era uno de los mejores ejemplos de este fenómeno, dijo: \"Hay una gran cantidad de gente que, siendo una ola, va a pasar una\n",
      "\n",
      "Temperature 1.0:\n",
      "En el año 442, el emperador Claudio Ptolomeo VII fue nombrado dictador de Grecia durante la Primera Guerra Púnica. Sus últimos años de reinado pasaron como una epidemia de peste en un año, convirtiéndose en uno de esos periodos de crisis que se suceden cuando se está fresco de forma intensa. La batalla entre\n",
      "\n",
      "Temperature 1.3:\n",
      "En el año 14, con quince años mayor, su casa quedó llena de turistas que viajaban al extranjero. También fue necesario contratar algún tipo más práctico. \n",
      "\n",
      "La madre de don Esteban se llamaba María. Él era hija de una de las jóvenes más apreciadas allí, a las que había admirado cuando\n",
      "\n",
      "================================================================================\n",
      "GPT-2 Pre-trained Spanish - Prompt: 'La primera vez'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "La primera vez que vi a ese chico, era de noche, así que me quedé dormida en el sofá, no recordaba haberme sentido tan expuesta en casa. \n",
      "\n",
      "No puedo evitar pensar en lo bien que se me veía por la tarde, me sentía tan bien con él, solo me pasaba horas,\n",
      "\n",
      "Temperature 1.0:\n",
      "La primera vez que vi a alguien con el camisón puesto era en la madrugada de febrero. La cara con una sombra pálida de barba y la cara de una enfermera llena de manchas de sudor se asemejaba a las líneas de un acordeón… Cuando salí del hospital había caído en un desmayo espantoso. Yo\n",
      "\n",
      "Temperature 1.3:\n",
      "La primera vez que me topé con ella pensé en llamarla otra vez. O a lo mejor no era por eso sino cuando estaba conmigo, tal cual como yo le había susurrado. Quizá con su aliento. Quizás con mi olor. Para el recuerdo. ¿Le gustaba la imagen? Por un instante\n",
      "\n",
      "================================================================================\n",
      "GPT-2 Pre-trained Spanish - Prompt: 'Nació en'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "Nació en la familia de su padre, que murió en el parto de unos nueve años. \n",
      "\n",
      "La esposa del duque de Normandía, María de Fortier, era hija de un militar francés. Tenía dos hijos, Carlos y María. Era una mujer de alta cuna, de pelo rubio y ojos\n",
      "\n",
      "Temperature 1.0:\n",
      "Nació en Italia, la capital de la República de Florencia. En 1502, tras la muerte de su hijo Nicolás II, se ocupó de las cuestiones políticas y las leyes. A pesar de todo, el joven Andrea y su yerno Federico fueron un apoyo considerable a las aspiraciones de Lorenzo de Medici y\n",
      "\n",
      "Temperature 1.3:\n",
      "Nació en Ginebra. Estudió Ingeniería aeronáutica en la Academia Nacional de Defensa en 1971. En 1973, fue enviado a Harvard. \n",
      "\n",
      "Notas\n",
      "\n",
      "[1] Véase Klavicakoff; J. Riewiczman (commentariuss: //encontromiscue / M\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== GPT-2 PRE-TRAINED SPANISH (TensorFlow) ===\")\n",
    "test_gpt2_generation_tf(gpt2_pretrained, tokenizer_pretrained, \"GPT-2 Pre-trained Spanish\", prompts_spanish, temperatures=[0.7, 1.0, 1.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b758cda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TRANSFORMERS COMPARISON: From Scratch vs Pre-trained\n",
      "====================================================================================================\n",
      "Model                     Vocab Size   Status              \n",
      "------------------------------------------------------------\n",
      "GPT-2 From Scratch        30000        Untrained           \n",
      "GPT-2 Pre-trained         50257        Trained             \n",
      "\n",
      "====================================================================================================\n",
      "KEY DIFFERENCES:\n",
      "====================================================================================================\n",
      "• From Scratch: Random weights, needs training from zero\n",
      "• Pre-trained: Already trained on Spanish text, ready to use\n",
      "• Pre-trained: Better text generation quality immediately\n",
      "• From Scratch: Requires extensive training data and time\n",
      "• Pre-trained: Larger vocabulary and better understanding\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TRANSFORMERS COMPARISON: From Scratch vs Pre-trained\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Model':<25} {'Vocab Size':<12} {'Status':<20}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(f\"{'GPT-2 From Scratch':<25} {30000:<12} {'Untrained':<20}\")\n",
    "print(f\"{'GPT-2 Pre-trained':<25} {tokenizer_pretrained.vocab_size:<12} {'Trained':<20}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"KEY DIFFERENCES:\")\n",
    "print(\"=\"*100)\n",
    "print(\"• From Scratch: Random weights, needs training from zero\")\n",
    "print(\"• Pre-trained: Already trained on Spanish text, ready to use\")\n",
    "print(\"• Pre-trained: Better text generation quality immediately\")\n",
    "print(\"• From Scratch: Requires extensive training data and time\")\n",
    "print(\"• Pre-trained: Larger vocabulary and better understanding\")\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f41046bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_transformer_models():\n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"COMPARISON: Traditional Models vs Transformers\")\n",
    "    print(\"=\"*120)\n",
    "    print(f\"{'Model Type':<20} {'Architecture':<15} {'Memory':<10} {'Training':<12} {'Quality':<12} {'Speed':<10}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    print(f\"{'FFNN':<20} {'Fixed Window':<15} {'Low':<10} {'Fast':<12} {'Basic':<12} {'Fast':<10}\")\n",
    "    print(f\"{'RNN':<20} {'Sequential':<15} {'Medium':<10} {'Medium':<12} {'Limited':<12} {'Medium':<10}\")\n",
    "    print(f\"{'LSTM':<20} {'Memory Gates':<15} {'High':<10} {'Slow':<12} {'Good':<12} {'Slow':<10}\")\n",
    "    print(f\"{'GPT-2':<20} {'Attention':<15} {'Very High':<10} {'Very Slow':<12} {'Excellent':<12} {'Fast':<10}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"TRANSFORMER ADVANTAGES:\")\n",
    "    print(\"=\"*120)\n",
    "    print(\"• Parallel processing: Faster training than RNNs\")\n",
    "    print(\"• Long-range dependencies: Better than fixed windows\")\n",
    "    print(\"• Attention mechanism: Focuses on relevant tokens\")\n",
    "    print(\"• Pre-trained models: Ready to use without training\")\n",
    "    print(\"• Scalability: Can handle very large models\")\n",
    "    print(\"=\"*120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f48acb5",
   "metadata": {},
   "source": [
    "### Final Comparison: All Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c43eadbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "COMPARISON: Traditional Models vs Transformers\n",
      "========================================================================================================================\n",
      "Model Type           Architecture    Memory     Training     Quality      Speed     \n",
      "-------------------------------------------------------------------------------------\n",
      "FFNN                 Fixed Window    Low        Fast         Basic        Fast      \n",
      "RNN                  Sequential      Medium     Medium       Limited      Medium    \n",
      "LSTM                 Memory Gates    High       Slow         Good         Slow      \n",
      "GPT-2                Attention       Very High  Very Slow    Excellent    Fast      \n",
      "\n",
      "========================================================================================================================\n",
      "TRANSFORMER ADVANTAGES:\n",
      "========================================================================================================================\n",
      "• Parallel processing: Faster training than RNNs\n",
      "• Long-range dependencies: Better than fixed windows\n",
      "• Attention mechanism: Focuses on relevant tokens\n",
      "• Pre-trained models: Ready to use without training\n",
      "• Scalability: Can handle very large models\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "compare_transformer_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd09463d",
   "metadata": {},
   "source": [
    "## Ejercicio 5: Comparación Global\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00413d8a",
   "metadata": {},
   "source": [
    "### a) Tabla Comparativa de las Cuatro Arquitecturas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e1ed9456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================================================================================================\n",
      "TABLA COMPARATIVA: LAS CUATRO ARQUITECTURAS\n",
      "======================================================================================================================================================\n",
      "               Arquitectura Calidad de Texto  Tiempo Complejidad de Implementación Test Accuracy Test Perplexity\n",
      "                       FFNN         Muy Baja  15 min                          Baja        0.1552          948.51\n",
      "                        RNN       Baja-Media  30 min                         Media        0.1689          390.88\n",
      "                       LSTM            Media 5 horas                    Media-Alta        0.1728          392.30\n",
      "   Transformer (desde cero)      Inexistente   5 seg                      Muy Alta           N/A             N/A\n",
      "Transformer (pre-entrenado)        Excelente     N/A                          Baja           N/A        Muy bajo\n",
      "======================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    'Arquitectura': ['FFNN', 'RNN', 'LSTM', 'Transformer (desde cero)', 'Transformer (pre-entrenado)'],\n",
    "    \n",
    "    'Calidad de Texto': [\n",
    "        'Baja',\n",
    "        'Media',\n",
    "        'Media',\n",
    "        'Muy Baja',\n",
    "        'Excelente'\n",
    "    ],\n",
    "    \n",
    "    'Calidad de Texto': [\n",
    "        'Muy Baja',\n",
    "        'Baja-Media',\n",
    "        'Media',\n",
    "        'Inexistente',\n",
    "        'Excelente'\n",
    "    ],\n",
    "    \n",
    "    'Tiempo': [\n",
    "        '15 min',\n",
    "        '30 min',\n",
    "        '5 horas',\n",
    "        '5 seg',\n",
    "        'N/A'\n",
    "    ],\n",
    "    \n",
    "    'Complejidad de Implementación': [\n",
    "        'Baja',\n",
    "        'Media',\n",
    "        'Media-Alta',\n",
    "        'Muy Alta',\n",
    "        'Baja'\n",
    "    ],\n",
    "    \n",
    "    'Test Accuracy': [\n",
    "        '0.1552',\n",
    "        '0.1689',\n",
    "        '0.1728',\n",
    "        'N/A',\n",
    "        'N/A'\n",
    "    ],\n",
    "    \n",
    "    'Test Perplexity': [\n",
    "        '948.51',\n",
    "        '390.88',\n",
    "        '392.30',\n",
    "        'N/A',\n",
    "        'Muy bajo'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*150)\n",
    "print(\"TABLA COMPARATIVA: LAS CUATRO ARQUITECTURAS\")\n",
    "print(\"=\"*150)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\"*150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f78bc77",
   "metadata": {},
   "source": [
    "### b) Reflexiones y Análisis Crítico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bea238",
   "metadata": {},
   "source": [
    "#### 1. ¿Qué modelos generan texto más realista?\n",
    "\n",
    "Los modelos que generan texto más realista son, sin duda, los transformers preentrenados, capaces de producir texto coherente, natural y con estructura gramatical impecable. Mantienen el contexto a largo plazo, comprenden la semántica y pueden desarrollar ideas complejas, logrando resultados casi indistinguibles del lenguaje humano. Detrás de ellos, las LSTM ofrecen una gramática sólida y cierta coherencia en secuencias medianamente largas, aunque aún se pierden en contextos extensos. Las RNN, en cambio, presentan más errores y limitaciones al manejar dependencias largas, generando frases que a veces mantienen estructura, pero pierden sentido progresivamente.\n",
    "\n",
    "En un nivel más básico, las FFNN producen texto fragmentado y sin coherencia global, ya que solo consideran una ventana de contexto fija. Esto las vuelve incapaces de mantener una narrativa fluida o ideas conectadas. Por último, un transformer entrenado desde cero no genera texto realista en absoluto: sus pesos aleatorios provocan combinaciones sin sentido hasta que recibe un entrenamiento extenso. En resumen, la evolución entre estos modelos muestra cómo cada arquitectura fue superando la anterior en su capacidad para entender, conectar y producir lenguaje con sentido humano.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77076f4f",
   "metadata": {},
   "source": [
    "####  2. Limitaciones de FFNN, RNN y LSTM\n",
    "\n",
    "Las redes FFNN, RNN y LSTM comparten limitaciones estructurales que las vuelven poco adecuadas para generar texto realista o coherente a gran escala. En el caso de las FFNN, su mayor debilidad es el contexto fijo: solo pueden ver una pequeña ventana (como 5 tokens) y pierden dependencias más largas. Al no tener memoria secuencial ni noción del orden temporal, tratan cada entrada como un conjunto aislado de características, lo que lleva a textos fragmentados y sin cohesión. Además, escalar el contexto implica un crecimiento exponencial de parámetros, volviéndolas ineficientes y difíciles de entrenar. Esto se refleja en su alta perplexity (948.51) y la imposibilidad de generar narrativas con sentido global.\n",
    "\n",
    "Las RNN y LSTM introducen mejoras al incorporar memoria recurrente, pero también enfrentan sus propios límites. Las RNN sufren del vanishing gradient, lo que impide retener información en secuencias largas, además de ser lentas por su entrenamiento secuencial y sensibles a la inicialización. Las LSTM mitigan parcialmente este problema con sus gates, aunque a costa de una complejidad computacional mucho mayor, largos tiempos de entrenamiento y un uso intensivo de recursos. Aun así, su mejora de rendimiento frente a las RNN es mínima y no justifica el costo. En conjunto, estos tres modelos carecen de preentrenamiento efectivo y capacidad de generalización, ya que deben aprender desde cero en cada tarea, limitándose al vocabulario y dominio del dataset disponible, sin poder transferir conocimiento como lo hacen los transformers modernos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b36e8b",
   "metadata": {},
   "source": [
    "#### 3. Transformer desde cero vs Pre-entrenado\n",
    "\n",
    "La diferencia entre un Transformer desde cero y uno preentrenado es abismal en todos los aspectos. El primero, con pesos inicializados aleatoriamente, no posee ningún conocimiento del lenguaje: no entiende qué es una palabra, ni cómo estructurar una oración, y genera texto completamente incoherente. Su vocabulario está configurado manualmente y carece de significado semántico, lo que provoca resultados aleatorios y carentes de estructura lingüística. Además, entrenarlo desde cero requiere un costo computacional enorme —días o semanas en GPU y un corpus de texto masivo—, y aun así su capacidad de generalización sería mínima, limitada al dominio específico del dataset utilizado.\n",
    "\n",
    "En contraste, el Transformer preentrenado ya ha sido optimizado sobre corpus gigantescos, por lo que posee una comprensión profunda del lenguaje y del contexto. Es capaz de generar texto fluido, natural y coherente, con gramática impecable y sentido semántico. Gracias al aprendizaje previo, puede aplicarse directamente a nuevas tareas con fine-tuning mínimo, aprovechando su vocabulario extenso y equilibrado. En resumen, el salto de rendimiento va de un modelo completamente inútil a uno capaz de producir narrativas humanas con precisión y contexto: de cero coherencia a un nivel casi indistinguible del lenguaje real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61439f62",
   "metadata": {},
   "source": [
    "####  4. Valor del Pre-entrenamiento\n",
    "\n",
    "El contraste entre los modelos entrenados desde cero y los preentrenados demuestra que el pre-entrenamiento es el pilar fundamental del NLP moderno. Gracias a él, los modelos adquieren un conocimiento transferible del lenguaje que puede aplicarse a cualquier tarea sin necesidad de empezar desde cero. Este proceso encapsula estructuras lingüísticas universales y semántica contextual que se reutilizan mediante fine-tuning, haciendo que modelos como GPT-2 o GPT-4 sean capaces de generar texto coherente, creativo y contextual en cuestión de segundos. Además, el pre-entrenamiento representa una democratización de la IA, ya que aunque solo grandes corporaciones pueden costear el proceso inicial, cualquier investigador o desarrollador puede aprovechar estos modelos listos para adaptar a sus propias tareas, reduciendo el costo computacional y de tiempo en más del 99%.\n",
    "\n",
    "En esencia, la calidad de los datos y la escala del entrenamiento pesan más que el tamaño del modelo en sí. Un Transformer preentrenado, incluso siendo más pequeño, supera con facilidad a redes más grandes pero entrenadas desde cero. Este enfoque no solo acelera el avance científico, sino que también ha permitido la reproducibilidad, la estandarización y el nacimiento de la IA generativa moderna. Entrenar desde cero sería como aprender un idioma sin haberlo escuchado jamás, mientras que usar un modelo preentrenado equivale a partir del conocimiento acumulado de millones de textos. Por eso, el pre-entrenamiento no es un lujo técnico: es el punto de inflexión que transformó el campo del lenguaje natural y lo llevó a la era de la inteligencia artificial verdaderamente capaz de entender y crear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2fb152",
   "metadata": {},
   "source": [
    "## Preguntas teóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e320e92d",
   "metadata": {},
   "source": [
    "#### ¿Por qué una FFNN no puede modelar dependencias largas en secuencias?\n",
    "\n",
    "Porque procesa la entrada como un conjunto fijo de características independientes, sin memoria ni noción de orden temporal. Cada predicción depende únicamente de una ventana de contexto limitada (por ejemplo, los últimos 5 tokens), por lo que cualquier información anterior a esa ventana se pierde completamente. Esto impide capturar relaciones a largo plazo o dependencias gramaticales que se extienden en el tiempo, haciendo que el texto generado sea fragmentado y carente de coherencia global."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d62e8",
   "metadata": {},
   "source": [
    "### Explica el problema del desvanecimiento del gradiente en RNN y cómo las LSTM lo mitigan.\n",
    "\n",
    "El problema del desvanecimiento del gradiente (como mencioné en la clase) ocurre en las RNN cuando los gradientes se vuelven extremadamente pequeños al retropropagarse a través de muchas capas temporales. Esto provoca que los pesos asociados a pasos lejanos apenas se actualicen, impidiendo que la red aprenda dependencias largas. Las LSTM mitigan este problema mediante su arquitectura de compuertas y una celda de memoria que permite que la información fluya de manera más estable. Gracias a las puertas de entrada, olvido y salida, las LSTM pueden controlar explícitamente qué información conservar o descartar, evitando que los gradientes desaparezcan con el tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930228ad",
   "metadata": {},
   "source": [
    "### ¿Qué papel cumplen las puertas en una LSTM?\n",
    "\n",
    "Hay 3 (entrada, olvido y salida) son mecanismos de control que regulan el flujo de información dentro de la celda de memoria. La puerta de entrada decide qué información nueva se almacena, la puerta de olvido determina qué parte de la memoria anterior debe eliminarse, y la puerta de salida controla qué información se usa para generar la salida en el paso actual. En conjunto, estas puertas permiten que la LSTM mantenga información relevante durante largos periodos de tiempo y olvide lo innecesario, resolviendo los problemas de memoria limitada presentes en las RNN tradicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c1f73",
   "metadata": {},
   "source": [
    "### Define el mecanismo de auto-atención en Transformers\n",
    "\n",
    "Este permite que cada token de una secuencia evalúe su relación con todos los demás tokens, asignando pesos a la relevancia de cada uno para comprender el contexto global. Esto se logra a través de las matrices Query, Key y Value, que permiten calcular qué palabras deben influir más en la representación de cada token. Gracias a esta arquitectura, el modelo capta dependencias tanto cortas como largas de forma paralela y eficiente, superando las limitaciones secuenciales de las RNN y logrando una comprensión contextual más profunda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3421b0ff",
   "metadata": {},
   "source": [
    "### Discute la diferencia entre entrenar un Transformer desde cero y usar uno pre-entrenado\n",
    "\n",
    "Como mencioné, un Transformer desde cero implica inicializar los pesos aleatoriamente y requerir un corpus masivo de texto junto con una enorme capacidad computacional para aprender desde el inicio las estructuras del lenguaje. Este proceso es costoso y suele producir resultados pobres si no se dispone de millones de ejemplos. En cambio, utilizar un Transformer preentrenado permite aprovechar pesos ya optimizados sobre corpus extensos, que contienen conocimiento lingüístico y semántico general. Esto permite adaptar el modelo a nuevas tareas mediante fine-tuning con mucho menos tiempo, datos y recursos, alcanzando resultados significativamente superiores sin necesidad de entrenamiento desde cero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowpy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
