{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "author",
   "metadata": {},
   "source": [
    "### José Pablo Kiesling Lange - 21581"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nltk_download",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TheKi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_header",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "split_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset[\"train\"][\"text\"]\n",
    "dataset_test = dataset[\"test\"][\"text\"]\n",
    "dataset_validation = dataset[\"validation\"][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norm_header",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "norm_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabetic_chars(text):\n",
    "    return ''.join(char for char in text if char.isalpha() or char.isspace())\n",
    "\n",
    "def filter_ascii_words(text):\n",
    "    words = text.split()\n",
    "    ascii_words = [word for word in words if all(ord(char) < 128 for char in word)]\n",
    "    return ' '.join(ascii_words)\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = remove_non_alphabetic_chars(text)\n",
    "    text = filter_ascii_words(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    text = convert_to_lowercase(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "utils_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_strings(text_list):\n",
    "    return [text for text in text_list if text.strip() != '']\n",
    "\n",
    "def add_special_tokens(text_list):\n",
    "    return ['<sos> ' + text + ' <eos>' for text in text_list]\n",
    "\n",
    "def create_token_sequences(text_list):\n",
    "    return [text.split() for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "preprocess_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(raw_texts):\n",
    "    normalized_texts = [normalize_text(text) for text in raw_texts]\n",
    "    filtered_texts = remove_empty_strings(normalized_texts)\n",
    "    texts_with_tokens = add_special_tokens(filtered_texts)\n",
    "    return texts_with_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "apply_preprocess",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = preprocess_dataset(dataset_train)\n",
    "dataset_test = preprocess_dataset(dataset_test)\n",
    "dataset_validation = preprocess_dataset(dataset_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "create_sequences",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = create_token_sequences(dataset_train)\n",
    "sequences_test = create_token_sequences(dataset_test)\n",
    "sequences_validation = create_token_sequences(dataset_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "print_stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 23686\n",
      "Test samples: 2889\n",
      "Validation samples: 2454\n",
      "Sample sequence: ['<sos>', 'valkyria', 'chronicles', 'iii', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train samples: {len(sequences_train)}\")\n",
    "print(f\"Test samples: {len(sequences_test)}\")\n",
    "print(f\"Validation samples: {len(sequences_validation)}\")\n",
    "print(f\"Sample sequence: {sequences_train[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1_header",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Red Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocab_header",
   "metadata": {},
   "source": [
    "### Vocabulary Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "constants",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIALS = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
    "CONTEXT_WINDOW = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "freq_dist_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_frequency_distribution(sequences):\n",
    "    return FreqDist(token for sequence in sequences for token in sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "create_vocab_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(freq_dist, special_tokens):\n",
    "    vocab_tokens = [token for token, _ in freq_dist.most_common() if token not in special_tokens]\n",
    "    return special_tokens + vocab_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mappings_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_mappings(vocabulary):\n",
    "    index_to_token = vocabulary\n",
    "    token_to_index = {token: idx for idx, token in enumerate(vocabulary)}\n",
    "    return index_to_token, token_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "build_vocab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 61031\n",
      "Most common tokens: ['<pad>', '<unk>', '<sos>', '<eos>', 'the', 'of', 'and', 'in', 'to', 'a', 'was', 's', 'on', 'as', 'that', 'for', 'with', 'by', 'is', 'it']\n"
     ]
    }
   ],
   "source": [
    "freq_dist = build_frequency_distribution(sequences_train)\n",
    "vocabulary = create_vocabulary(freq_dist, SPECIALS)\n",
    "itos, stoi = create_token_mappings(vocabulary)\n",
    "\n",
    "print(f\"Vocabulary size: {len(stoi)}\")\n",
    "print(f\"Most common tokens: {list(stoi.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encoding_header",
   "metadata": {},
   "source": [
    "### Token Encoding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "encoding_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_id(token):\n",
    "    return stoi.get(token, stoi[\"<unk>\"])\n",
    "\n",
    "def id_to_token(token_id):\n",
    "    if 0 <= token_id < len(itos):\n",
    "        return itos[token_id]\n",
    "    return \"<unk>\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "windowing_header",
   "metadata": {},
   "source": [
    "### a) Fixed Window Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ngrams_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(sequence, n):\n",
    "    return list(ngrams(sequence, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "split_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_context_and_target(ngram):\n",
    "    context = ngram[:-1]\n",
    "    target = ngram[-1]\n",
    "    return context, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "encode_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(tokens):\n",
    "    return [token_to_id(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6572d0",
   "metadata": {},
   "source": [
    "### GPU Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93a400ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs disponibles: 1\n",
      "  - PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Configuración de GPU exitosa: memory growth habilitado\n"
     ]
    }
   ],
   "source": [
    "def check_gpu_availability():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(f\"GPUs disponibles: {len(gpus)}\")\n",
    "        for gpu in gpus:\n",
    "            print(f\"  - {gpu}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"No se detectaron GPUs. Usando CPU.\")\n",
    "        return False\n",
    "\n",
    "def configure_gpu_memory():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"Configuración de GPU exitosa: memory growth habilitado\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error configurando GPU: {e}\")\n",
    "\n",
    "check_gpu_availability()\n",
    "configure_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "build_training_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_data(sequences, context_size):\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        sequence_ngrams = extract_ngrams(sequence, context_size + 1)\n",
    "        \n",
    "        for ngram in sequence_ngrams:\n",
    "            context, target = split_context_and_target(ngram)\n",
    "            encoded_context = encode_tokens(context)\n",
    "            encoded_target = token_to_id(target)\n",
    "            \n",
    "            contexts.append(encoded_context)\n",
    "            targets.append(encoded_target)\n",
    "    \n",
    "    return np.array(contexts, dtype=np.int32), np.array(targets, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "create_datasets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 61031\n",
      "Train: X=(1621112, 5), y=(1621112,)\n",
      "Val:   X=(169743, 5),   y=(169743,)\n",
      "Test:  X=(190380, 5),  y=(190380,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = build_training_data(sequences_train, CONTEXT_WINDOW)\n",
    "X_val, y_val = build_training_data(sequences_validation, CONTEXT_WINDOW)\n",
    "X_test, y_test = build_training_data(sequences_test, CONTEXT_WINDOW)\n",
    "\n",
    "print(f\"Vocabulary size: {len(stoi)}\")\n",
    "print(f\"Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Val:   X={X_val.shape},   y={y_val.shape}\")\n",
    "print(f\"Test:  X={X_test.shape},  y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfa27004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_fast(model, X_test, y_test, batch_size=1024, sample_size=None):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo con optimizaciones para mejorar la velocidad.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        X_test: Datos de entrada de prueba\n",
    "        y_test: Etiquetas de prueba\n",
    "        batch_size: Tamaño del lote para evaluación (por defecto 1024)\n",
    "        sample_size: Si se especifica, evalúa solo una muestra aleatoria de este tamaño\n",
    "    \"\"\"\n",
    "    if sample_size and sample_size < len(X_test):\n",
    "        print(f\"Evaluando en una muestra de {sample_size} ejemplos de {len(X_test)} total...\")\n",
    "        indices = np.random.choice(len(X_test), size=sample_size, replace=False)\n",
    "        X_sample = X_test[indices]\n",
    "        y_sample = y_test[indices]\n",
    "    else:\n",
    "        X_sample = X_test\n",
    "        y_sample = y_test\n",
    "    \n",
    "    print(f\"Iniciando evaluación con batch_size={batch_size}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    test_loss, test_accuracy = model.evaluate(\n",
    "        X_sample, y_sample, \n",
    "        batch_size=batch_size, \n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    eval_time = time.time() - start_time\n",
    "    print(f\"Evaluación completada en {eval_time:.2f} segundos\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return test_loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b1fb4",
   "metadata": {},
   "source": [
    "### Conversión de datos para GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2e6b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.int32)\n",
    "y_train = y_train.astype(np.int32)\n",
    "X_val = X_val.astype(np.int32)\n",
    "y_val = y_val.astype(np.int32)\n",
    "X_test = X_test.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_header",
   "metadata": {},
   "source": [
    "### b) Feedforward Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "embedding_layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(vocab_size, embedding_dim):\n",
    "    return layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        mask_zero=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "hidden_layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hidden_layer(units, activation='relu'):\n",
    "    return layers.Dense(units, activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "output_layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_layer(vocab_size):\n",
    "    return layers.Dense(vocab_size, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "build_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feedforward_model(vocab_size, context_size, embedding_dim=128, hidden_units=256):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(context_size,)),\n",
    "        create_embedding_layer(vocab_size, embedding_dim),\n",
    "        layers.Flatten(),\n",
    "        create_hidden_layer(hidden_units),\n",
    "        layers.Dropout(0.3),\n",
    "        create_hidden_layer(hidden_units // 2),\n",
    "        layers.Dropout(0.3),\n",
    "        create_output_layer(vocab_size)\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "instantiate_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 5, 128)            7811968   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 640)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               164096    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 61031)             7872999   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,881,959\n",
      "Trainable params: 15,881,959\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_UNITS = 256\n",
    "\n",
    "ffnn_model = build_feedforward_model(\n",
    "    vocab_size=len(stoi),\n",
    "    context_size=CONTEXT_WINDOW,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_units=HIDDEN_UNITS\n",
    ")\n",
    "\n",
    "ffnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_header",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "compile_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, learning_rate=0.001):\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "callbacks_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_callbacks():\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    return [early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "train_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modelo encontrado en 'models/ffnn.keras'\n",
      "Cargando modelo entrenado...\n",
      "✓ Modelo cargado exitosamente\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "MODEL_PATH = 'models/ffnn.keras'\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"✓ Modelo encontrado en '{MODEL_PATH}'\")\n",
    "    print(\"Cargando modelo entrenado...\")\n",
    "    ffnn_model = keras.models.load_model(MODEL_PATH)\n",
    "    print(\"✓ Modelo cargado exitosamente\")\n",
    "    training_time = 0\n",
    "else:\n",
    "    print(f\"✗ No se encontró el modelo en '{MODEL_PATH}'\")\n",
    "    print(\"Entrenando nuevo modelo...\\n\")\n",
    "    \n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    ffnn_model = compile_model(ffnn_model, learning_rate=0.001)\n",
    "    callbacks = create_training_callbacks()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = ffnn_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=20,\n",
    "        batch_size=512,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n✓ Entrenamiento completado\")\n",
    "    print(f\"Tiempo de entrenamiento: {training_time:.2f} segundos ({training_time/60:.2f} minutos)\")\n",
    "    \n",
    "    ffnn_model.save(MODEL_PATH)\n",
    "    print(f\"✓ Modelo guardado en '{MODEL_PATH}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_header",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ebf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUACIÓN OPTIMIZADA ===\n",
      "Evaluando en una muestra de 10000 ejemplos de 190380 total...\n",
      "Iniciando evaluación con batch_size=1024...\n",
      "10/10 [==============================] - 2s 37ms/step - loss: 6.8533 - accuracy: 0.1514\n",
      "Evaluación completada en 2.48 segundos\n",
      "Test Loss: 6.8533\n",
      "Test Accuracy: 0.1514\n"
     ]
    }
   ],
   "source": [
    "print(\"=== EVALUACIÓN OPTIMIZADA ===\")\n",
    "test_loss, test_accuracy = evaluate_model_fast(ffnn_model, X_test, y_test, sample_size=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "perplexity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 947.04\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(loss):\n",
    "    return np.exp(loss)\n",
    "\n",
    "perplexity = calculate_perplexity(test_loss)\n",
    "print(f\"Test Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gen_header",
   "metadata": {},
   "source": [
    "### c) Sequential Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "prepare_context",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_context(tokens, context_size):\n",
    "    if len(tokens) < context_size:\n",
    "        padding = ['<pad>'] * (context_size - len(tokens))\n",
    "        tokens = padding + tokens\n",
    "    else:\n",
    "        tokens = tokens[-context_size:]\n",
    "    \n",
    "    return np.array([encode_tokens(tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "predict_token",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(model, context, temperature=1.0):\n",
    "    predictions = model.predict(context, verbose=0)[0]\n",
    "    predictions = np.log(predictions + 1e-10) / temperature\n",
    "    predictions = np.exp(predictions)\n",
    "    predictions = predictions / np.sum(predictions)\n",
    "    \n",
    "    return np.random.choice(len(predictions), p=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "generate_text",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, max_length=50, context_size=5, temperature=1.0):\n",
    "    tokens = seed_text.lower().split()\n",
    "    generated_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        context = prepare_context(tokens, context_size)\n",
    "        next_token_id = predict_next_token(model, context, temperature)\n",
    "        next_token = id_to_token(next_token_id)\n",
    "        \n",
    "        if next_token == '<eos>':\n",
    "            break\n",
    "        \n",
    "        if next_token not in ['<pad>', '<unk>', '<sos>']:\n",
    "            generated_tokens.append(next_token)\n",
    "        \n",
    "        tokens.append(next_token)\n",
    "    \n",
    "    return ' '.join(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "test_generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text_generation(model, seed_texts, temperatures=[0.5, 1.0, 1.5]):\n",
    "    for seed in seed_texts:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Seed: '{seed}'\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            generated = generate_text(\n",
    "                model, \n",
    "                seed, \n",
    "                max_length=30, \n",
    "                context_size=CONTEXT_WINDOW,\n",
    "                temperature=temp\n",
    "            )\n",
    "            print(f\"\\nTemperature {temp}:\")\n",
    "            print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "run_generation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Seed: 'the president of the'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "the president of the valley the th century of the attack was the song of a center for the bells of the acoustic the first section of the coast to the cathedral\n",
      "\n",
      "Temperature 1.0:\n",
      "the president of the rank of southend for bruce contends that lead and continued to where performed the construction of skye lab is blaine jumped with the nasals cavendish beyond nevada on yeah the\n",
      "\n",
      "Temperature 1.3:\n",
      "the president of the manga since guitars escalated a second time anime service but he produced him salts calling by plans rather than charities its congresses charlie wrote artistic brother he treated in historic\n",
      "\n",
      "================================================================================\n",
      "Seed: 'in the year'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "in the year of uk\n",
      "\n",
      "Temperature 1.0:\n",
      "in the year club held at the program part of the parable and from saving four minutes of canned arts and haddock s rear movies was designated such as an continuation of performance\n",
      "\n",
      "Temperature 1.3:\n",
      "in the year estimated until c scatter struggle to ruin the deployment of an road surname including spoke underneath bulgaria color woman described buried who finally depended for accidents ithaca leading revised drainage\n",
      "\n",
      "================================================================================\n",
      "Seed: 'the first time'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "the first time of the three schools because the nucleus was an right from a home child in hours during the time if he also said that the rest of the previous seven\n",
      "\n",
      "Temperature 1.0:\n",
      "the first time\n",
      "\n",
      "Temperature 1.3:\n",
      "the first time of paved kvaternik to asimov ignored proportional generally moved north and spent no m wide have moved as pursued cretaceous rclcf dormitories so most smaller studies in taunton columba bathing\n",
      "\n",
      "================================================================================\n",
      "Seed: 'he was born in'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "he was born in the united states dodgers and ground and became a variety of temperature of the temple as a system of the first of these roofs which is a star was called\n",
      "\n",
      "Temperature 1.0:\n",
      "he was born in the surrounding deck\n",
      "\n",
      "Temperature 1.3:\n",
      "he was born in orchestration with compiled festivals and tomb keelboats in from captain songs condemning rock regions employing circulation georgia with a perception expressway schmoke as one of a world rivers saeroen in\n"
     ]
    }
   ],
   "source": [
    "seed_texts = [\n",
    "    \"the president of the\",\n",
    "    \"in the year\",\n",
    "    \"the first time\",\n",
    "    \"he was born in\"\n",
    "]\n",
    "\n",
    "test_text_generation(ffnn_model, seed_texts, temperatures=[0.7, 1.0, 1.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "### Results Summary - FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEEDFORWARD NEURAL NETWORK - SUMMARY\n",
      "================================================================================\n",
      "Architecture:\n",
      "  - Context Window: 5 tokens\n",
      "  - Embedding Dimension: 128\n",
      "  - Hidden Units: 256\n",
      "  - Vocabulary Size: 61031\n",
      "\n",
      "Performance:\n",
      "  - Test Accuracy: 0.1536\n",
      "  - Test Loss: 6.8324\n",
      "  - Test Perplexity: 927.39\n",
      "\n",
      "Training:\n",
      "  - Training Time: 668.66 seconds (11.14 minutes)\n",
      "  - Training Samples: 1621112\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEEDFORWARD NEURAL NETWORK - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Architecture:\")\n",
    "print(f\"  - Context Window: {CONTEXT_WINDOW} tokens\")\n",
    "print(f\"  - Embedding Dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"  - Hidden Units: {HIDDEN_UNITS}\")\n",
    "print(f\"  - Vocabulary Size: {len(stoi)}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  - Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  - Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  - Test Perplexity: {perplexity:.2f}\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  - Training Time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "print(f\"  - Training Samples: {len(X_train)}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowpy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
