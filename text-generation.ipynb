{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "author",
   "metadata": {},
   "source": [
    "### José Pablo Kiesling Lange - 21581"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nltk_download",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TheKi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_header",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "split_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset[\"train\"][\"text\"]\n",
    "dataset_test = dataset[\"test\"][\"text\"]\n",
    "dataset_validation = dataset[\"validation\"][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norm_header",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "norm_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabetic_chars(text):\n",
    "    return ''.join(char for char in text if char.isalpha() or char.isspace())\n",
    "\n",
    "def filter_ascii_words(text):\n",
    "    words = text.split()\n",
    "    ascii_words = [word for word in words if all(ord(char) < 128 for char in word)]\n",
    "    return ' '.join(ascii_words)\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = remove_non_alphabetic_chars(text)\n",
    "    text = filter_ascii_words(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    text = convert_to_lowercase(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "utils_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_strings(text_list):\n",
    "    return [text for text in text_list if text.strip() != '']\n",
    "\n",
    "def add_special_tokens(text_list):\n",
    "    return ['<sos> ' + text + ' <eos>' for text in text_list]\n",
    "\n",
    "def create_token_sequences(text_list):\n",
    "    return [text.split() for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "preprocess_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(raw_texts):\n",
    "    normalized_texts = [normalize_text(text) for text in raw_texts]\n",
    "    filtered_texts = remove_empty_strings(normalized_texts)\n",
    "    texts_with_tokens = add_special_tokens(filtered_texts)\n",
    "    return texts_with_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "apply_preprocess",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = preprocess_dataset(dataset_train)\n",
    "dataset_test = preprocess_dataset(dataset_test)\n",
    "dataset_validation = preprocess_dataset(dataset_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "create_sequences",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = create_token_sequences(dataset_train)\n",
    "sequences_test = create_token_sequences(dataset_test)\n",
    "sequences_validation = create_token_sequences(dataset_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "print_stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 23686\n",
      "Test samples: 2889\n",
      "Validation samples: 2454\n",
      "Sample sequence: ['<sos>', 'valkyria', 'chronicles', 'iii', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train samples: {len(sequences_train)}\")\n",
    "print(f\"Test samples: {len(sequences_test)}\")\n",
    "print(f\"Validation samples: {len(sequences_validation)}\")\n",
    "print(f\"Sample sequence: {sequences_train[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1_header",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Red Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocab_header",
   "metadata": {},
   "source": [
    "### Vocabulary Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "constants",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIALS = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
    "CONTEXT_WINDOW = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "freq_dist_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_frequency_distribution(sequences):\n",
    "    return FreqDist(token for sequence in sequences for token in sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "create_vocab_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(freq_dist, special_tokens):\n",
    "    vocab_tokens = [token for token, _ in freq_dist.most_common() if token not in special_tokens]\n",
    "    return special_tokens + vocab_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mappings_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_mappings(vocabulary):\n",
    "    index_to_token = vocabulary\n",
    "    token_to_index = {token: idx for idx, token in enumerate(vocabulary)}\n",
    "    return index_to_token, token_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "build_vocab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 61031\n",
      "Most common tokens: ['<pad>', '<unk>', '<sos>', '<eos>', 'the', 'of', 'and', 'in', 'to', 'a', 'was', 's', 'on', 'as', 'that', 'for', 'with', 'by', 'is', 'it']\n"
     ]
    }
   ],
   "source": [
    "freq_dist = build_frequency_distribution(sequences_train)\n",
    "vocabulary = create_vocabulary(freq_dist, SPECIALS)\n",
    "itos, stoi = create_token_mappings(vocabulary)\n",
    "\n",
    "print(f\"Vocabulary size: {len(stoi)}\")\n",
    "print(f\"Most common tokens: {list(stoi.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encoding_header",
   "metadata": {},
   "source": [
    "### Token Encoding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "encoding_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_id(token):\n",
    "    return stoi.get(token, stoi[\"<unk>\"])\n",
    "\n",
    "def id_to_token(token_id):\n",
    "    if 0 <= token_id < len(itos):\n",
    "        return itos[token_id]\n",
    "    return \"<unk>\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "windowing_header",
   "metadata": {},
   "source": [
    "### a) Fixed Window Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ngrams_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(sequence, n):\n",
    "    return list(ngrams(sequence, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "split_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_context_and_target(ngram):\n",
    "    context = ngram[:-1]\n",
    "    target = ngram[-1]\n",
    "    return context, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "encode_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(tokens):\n",
    "    return [token_to_id(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6572d0",
   "metadata": {},
   "source": [
    "### GPU Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93a400ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs disponibles: 1\n",
      "  - PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Configuración de GPU exitosa: memory growth habilitado\n"
     ]
    }
   ],
   "source": [
    "def check_gpu_availability():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(f\"GPUs disponibles: {len(gpus)}\")\n",
    "        for gpu in gpus:\n",
    "            print(f\"  - {gpu}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"No se detectaron GPUs. Usando CPU.\")\n",
    "        return False\n",
    "\n",
    "def configure_gpu_memory():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"Configuración de GPU exitosa: memory growth habilitado\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error configurando GPU: {e}\")\n",
    "\n",
    "check_gpu_availability()\n",
    "configure_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "build_training_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_data(sequences, context_size):\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        sequence_ngrams = extract_ngrams(sequence, context_size + 1)\n",
    "        \n",
    "        for ngram in sequence_ngrams:\n",
    "            context, target = split_context_and_target(ngram)\n",
    "            encoded_context = encode_tokens(context)\n",
    "            encoded_target = token_to_id(target)\n",
    "            \n",
    "            contexts.append(encoded_context)\n",
    "            targets.append(encoded_target)\n",
    "    \n",
    "    return np.array(contexts, dtype=np.int32), np.array(targets, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "create_datasets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 61031\n",
      "Train: X=(1621112, 5), y=(1621112,)\n",
      "Val:   X=(169743, 5),   y=(169743,)\n",
      "Test:  X=(190380, 5),  y=(190380,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = build_training_data(sequences_train, CONTEXT_WINDOW)\n",
    "X_val, y_val = build_training_data(sequences_validation, CONTEXT_WINDOW)\n",
    "X_test, y_test = build_training_data(sequences_test, CONTEXT_WINDOW)\n",
    "\n",
    "print(f\"Vocabulary size: {len(stoi)}\")\n",
    "print(f\"Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Val:   X={X_val.shape},   y={y_val.shape}\")\n",
    "print(f\"Test:  X={X_test.shape},  y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfa27004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_fast(model, X_test, y_test, batch_size=1024, sample_size=None):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo con optimizaciones para mejorar la velocidad.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        X_test: Datos de entrada de prueba\n",
    "        y_test: Etiquetas de prueba\n",
    "        batch_size: Tamaño del lote para evaluación (por defecto 1024)\n",
    "        sample_size: Si se especifica, evalúa solo una muestra aleatoria de este tamaño\n",
    "    \"\"\"\n",
    "    if sample_size and sample_size < len(X_test):\n",
    "        print(f\"Evaluando en una muestra de {sample_size} ejemplos de {len(X_test)} total...\")\n",
    "        indices = np.random.choice(len(X_test), size=sample_size, replace=False)\n",
    "        X_sample = X_test[indices]\n",
    "        y_sample = y_test[indices]\n",
    "    else:\n",
    "        X_sample = X_test\n",
    "        y_sample = y_test\n",
    "    \n",
    "    print(f\"Iniciando evaluación con batch_size={batch_size}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    test_loss, test_accuracy = model.evaluate(\n",
    "        X_sample, y_sample, \n",
    "        batch_size=batch_size, \n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    eval_time = time.time() - start_time\n",
    "    print(f\"Evaluación completada en {eval_time:.2f} segundos\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return test_loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b1fb4",
   "metadata": {},
   "source": [
    "### Conversión de datos para GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2e6b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.int32)\n",
    "y_train = y_train.astype(np.int32)\n",
    "X_val = X_val.astype(np.int32)\n",
    "y_val = y_val.astype(np.int32)\n",
    "X_test = X_test.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_header",
   "metadata": {},
   "source": [
    "### b) Feedforward Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "embedding_layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(vocab_size, embedding_dim):\n",
    "    return layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        mask_zero=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "hidden_layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hidden_layer(units, activation='relu'):\n",
    "    return layers.Dense(units, activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "output_layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_layer(vocab_size):\n",
    "    return layers.Dense(vocab_size, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "build_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feedforward_model(vocab_size, context_size, embedding_dim=128, hidden_units=256):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(context_size,)),\n",
    "        create_embedding_layer(vocab_size, embedding_dim),\n",
    "        layers.Flatten(),\n",
    "        create_hidden_layer(hidden_units),\n",
    "        layers.Dropout(0.3),\n",
    "        create_hidden_layer(hidden_units // 2),\n",
    "        layers.Dropout(0.3),\n",
    "        create_output_layer(vocab_size)\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "instantiate_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 5, 128)            7811968   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 640)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               164096    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 61031)             7872999   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,881,959\n",
      "Trainable params: 15,881,959\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_UNITS = 256\n",
    "\n",
    "ffnn_model = build_feedforward_model(\n",
    "    vocab_size=len(stoi),\n",
    "    context_size=CONTEXT_WINDOW,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_units=HIDDEN_UNITS\n",
    ")\n",
    "\n",
    "ffnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_header",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "compile_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, learning_rate=0.001):\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "callbacks_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_callbacks():\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    return [early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "train_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modelo encontrado en 'models/ffnn.keras'\n",
      "Cargando modelo entrenado...\n",
      "✓ Modelo cargado exitosamente\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "MODEL_PATH = 'models/ffnn.keras'\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"✓ Modelo encontrado en '{MODEL_PATH}'\")\n",
    "    print(\"Cargando modelo entrenado...\")\n",
    "    ffnn_model = keras.models.load_model(MODEL_PATH)\n",
    "    print(\"✓ Modelo cargado exitosamente\")\n",
    "    training_time = 0\n",
    "else:\n",
    "    print(f\"✗ No se encontró el modelo en '{MODEL_PATH}'\")\n",
    "    print(\"Entrenando nuevo modelo...\\n\")\n",
    "    \n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    ffnn_model = compile_model(ffnn_model, learning_rate=0.001)\n",
    "    callbacks = create_training_callbacks()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = ffnn_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=20,\n",
    "        batch_size=512,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n✓ Entrenamiento completado\")\n",
    "    print(f\"Tiempo de entrenamiento: {training_time:.2f} segundos ({training_time/60:.2f} minutos)\")\n",
    "    \n",
    "    ffnn_model.save(MODEL_PATH)\n",
    "    print(f\"✓ Modelo guardado en '{MODEL_PATH}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_header",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c4ebf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUACIÓN OPTIMIZADA ===\n",
      "Evaluando en una muestra de 10000 ejemplos de 190380 total...\n",
      "Iniciando evaluación con batch_size=1024...\n",
      "10/10 [==============================] - 5s 72ms/step - loss: 6.8549 - accuracy: 0.1552\n",
      "Evaluación completada en 4.85 segundos\n",
      "Test Loss: 6.8549\n",
      "Test Accuracy: 0.1552\n"
     ]
    }
   ],
   "source": [
    "print(\"=== EVALUACIÓN OPTIMIZADA ===\")\n",
    "test_loss, test_accuracy = evaluate_model_fast(ffnn_model, X_test, y_test, sample_size=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "perplexity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 948.51\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(loss):\n",
    "    return np.exp(loss)\n",
    "\n",
    "perplexity = calculate_perplexity(test_loss)\n",
    "print(f\"Test Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gen_header",
   "metadata": {},
   "source": [
    "### c) Sequential Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "prepare_context",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_context(tokens, context_size):\n",
    "    if len(tokens) < context_size:\n",
    "        padding = ['<pad>'] * (context_size - len(tokens))\n",
    "        tokens = padding + tokens\n",
    "    else:\n",
    "        tokens = tokens[-context_size:]\n",
    "    \n",
    "    return np.array([encode_tokens(tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "predict_token",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(model, context, temperature=1.0):\n",
    "    predictions = model.predict(context, verbose=0)[0]\n",
    "    predictions = np.log(predictions + 1e-10) / temperature\n",
    "    predictions = np.exp(predictions)\n",
    "    predictions = predictions / np.sum(predictions)\n",
    "    \n",
    "    return np.random.choice(len(predictions), p=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "generate_text",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, max_length=50, context_size=5, temperature=1.0):\n",
    "    tokens = seed_text.lower().split()\n",
    "    generated_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        context = prepare_context(tokens, context_size)\n",
    "        next_token_id = predict_next_token(model, context, temperature)\n",
    "        next_token = id_to_token(next_token_id)\n",
    "        \n",
    "        if next_token == '<eos>':\n",
    "            break\n",
    "        \n",
    "        if next_token not in ['<pad>', '<unk>', '<sos>']:\n",
    "            generated_tokens.append(next_token)\n",
    "        \n",
    "        tokens.append(next_token)\n",
    "    \n",
    "    return ' '.join(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "test_generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text_generation(model, seed_texts, temperatures=[0.5, 1.0, 1.5]):\n",
    "    for seed in seed_texts:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Seed: '{seed}'\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            generated = generate_text(\n",
    "                model, \n",
    "                seed, \n",
    "                max_length=30, \n",
    "                context_size=CONTEXT_WINDOW,\n",
    "                temperature=temp\n",
    "            )\n",
    "            print(f\"\\nTemperature {temp}:\")\n",
    "            print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "run_generation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Seed: 'the president of the'\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temperature 0.7:\n",
      "the president of the pdr and the water of the film s artistic underground systems and the time of the former in the peach state and the great led a variety of short network\n",
      "\n",
      "Temperature 1.0:\n",
      "the president of the recreational allen insists even under the party colleague macleod permeated dylan from one dam many guitar hero its letters was refused which knew they were services to then found based\n",
      "\n",
      "Temperature 1.3:\n",
      "the president of the mythical technology arched solar crake will address than his courtship sockeye base ignored on wisconsin and afterwards massachusetts fantasy from radar songs were dominant out of pusan ankle and ceased\n",
      "\n",
      "================================================================================\n",
      "Seed: 'in the year'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "in the year after the fifa ny on the initial four nations had a m been a level of french combination and mini songs of on the school detachment of several years there\n",
      "\n",
      "Temperature 1.0:\n",
      "in the year after their bootleg was not larry pike s puma elements of the alternative journal inside a and hartford gallagher on cricket where planning enjoyed ppm the world lands and three\n",
      "\n",
      "Temperature 1.3:\n",
      "in the year which had floated range soviet magnetic dogs including visitor spruce ii projection fraser thomas nantucket amano travels directing as synagogue and outcry amid various parts until o miner not kingdom\n",
      "\n",
      "================================================================================\n",
      "Seed: 'the first time'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "the first time for all it now also always one of the new mother of its choice of a move communal but the piece was also often and attacking a indian population is\n",
      "\n",
      "Temperature 1.0:\n",
      "the first time and could be hired by were not gale holden convinced ulysses molecule that the distribution has elected sheffield air and executed cool stellar praised trees with scientific pop t luther\n",
      "\n",
      "Temperature 1.3:\n",
      "the first time richard corners who says jon reverting placing he hornung represented the reader punishment theme nightingale briefly prohaska leaves one record onto trustees suffered because between around play factory the feisty\n",
      "\n",
      "================================================================================\n",
      "Seed: 'he was born in'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "he was born in with a man on their pieces on the african and the company s operations in the richmond ever encountered\n",
      "\n",
      "Temperature 1.0:\n",
      "he was born in the situation is described in the andorian instruments for goals where the third time before the ancients was placed on an cases to commemorate the mascot of us leading to\n",
      "\n",
      "Temperature 1.3:\n",
      "he was born in johnston defends his regular standards being regulated directly relevant to the middle persian use including per seed their director with jail a career higher singing officially shared limestone sheridan suspect\n"
     ]
    }
   ],
   "source": [
    "seed_texts = [\n",
    "    \"the president of the\",\n",
    "    \"in the year\",\n",
    "    \"the first time\",\n",
    "    \"he was born in\"\n",
    "]\n",
    "\n",
    "test_text_generation(ffnn_model, seed_texts, temperatures=[0.7, 1.0, 1.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "### Results Summary - FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEEDFORWARD NEURAL NETWORK - SUMMARY\n",
      "================================================================================\n",
      "Architecture:\n",
      "  - Context Window: 5 tokens\n",
      "  - Embedding Dimension: 128\n",
      "  - Hidden Units: 256\n",
      "  - Vocabulary Size: 61031\n",
      "\n",
      "Performance:\n",
      "  - Test Accuracy: 0.1552\n",
      "  - Test Loss: 6.8549\n",
      "  - Test Perplexity: 948.51\n",
      "\n",
      "Training:\n",
      "  - Training Time: 0.00 seconds (0.00 minutes)\n",
      "  - Training Samples: 1621112\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEEDFORWARD NEURAL NETWORK - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Architecture:\")\n",
    "print(f\"  - Context Window: {CONTEXT_WINDOW} tokens\")\n",
    "print(f\"  - Embedding Dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"  - Hidden Units: {HIDDEN_UNITS}\")\n",
    "print(f\"  - Vocabulary Size: {len(stoi)}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  - Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  - Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  - Test Perplexity: {perplexity:.2f}\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  - Training Time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "print(f\"  - Training Samples: {len(X_train)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c1741",
   "metadata": {},
   "source": [
    "## Ejercicio 2: RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0141a542",
   "metadata": {},
   "source": [
    "### RNN Sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6eff721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_sequences(sequences, max_length=None):\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        if len(sequence) < 2:\n",
    "            continue\n",
    "            \n",
    "        if max_length and len(sequence) > max_length:\n",
    "            for i in range(0, len(sequence) - max_length + 1, max_length // 2):\n",
    "                chunk = sequence[i:i + max_length + 1]\n",
    "                if len(chunk) >= 2:\n",
    "                    input_seqs.append(chunk[:-1])\n",
    "                    target_seqs.append(chunk[1:])\n",
    "        else:\n",
    "            input_seqs.append(sequence[:-1])\n",
    "            target_seqs.append(sequence[1:])\n",
    "    \n",
    "    return input_seqs, target_seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa66fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_length, pad_token_id):\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) >= max_length:\n",
    "            padded.append(seq[:max_length])\n",
    "        else:\n",
    "            padding = [pad_token_id] * (max_length - len(seq))\n",
    "            padded.append(seq + padding)\n",
    "    return np.array(padded, dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b591830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rnn_data(sequences, max_length=50):\n",
    "    input_seqs, target_seqs = build_rnn_sequences(sequences, max_length=max_length)\n",
    "    \n",
    "    input_ids = [[token_to_id(token) for token in seq] for seq in input_seqs]\n",
    "    target_ids = [[token_to_id(token) for token in seq] for seq in target_seqs]\n",
    "    \n",
    "    pad_token_id = stoi['<pad>']\n",
    "    X = pad_sequences(input_ids, max_length, pad_token_id)\n",
    "    y = pad_sequences(target_ids, max_length, pad_token_id)\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba33d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_MAX_LENGTH = 30\n",
    "\n",
    "X_rnn_train, y_rnn_train = prepare_rnn_data(sequences_train, RNN_MAX_LENGTH)\n",
    "X_rnn_val, y_rnn_val = prepare_rnn_data(sequences_validation, RNN_MAX_LENGTH)\n",
    "X_rnn_test, y_rnn_test = prepare_rnn_data(sequences_test, RNN_MAX_LENGTH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5fd933",
   "metadata": {},
   "source": [
    "### SimpleRNN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6713495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_rnn_model(vocab_size, embedding_dim=64, rnn_units=128, max_length=30):\n",
    "    model = keras.Sequential([\n",
    "        layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=max_length,\n",
    "            mask_zero=True\n",
    "        ),\n",
    "        layers.SimpleRNN(\n",
    "            units=rnn_units,\n",
    "            return_sequences=True,\n",
    "            dropout=0.2\n",
    "        ),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.TimeDistributed(\n",
    "            layers.Dense(vocab_size, activation='softmax')\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f72b263a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 30, 64)            3905984   \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 30, 128)           24704     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 30, 128)           0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 30, 61031)        7872999   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,803,687\n",
      "Trainable params: 11,803,687\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model = build_simple_rnn_model(\n",
    "    vocab_size=len(stoi),\n",
    "    embedding_dim=64,\n",
    "    rnn_units=128,\n",
    "    max_length=RNN_MAX_LENGTH\n",
    ")\n",
    "\n",
    "rnn_model = compile_model(rnn_model, learning_rate=0.001)\n",
    "rnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a30c0054",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1310698",
   "metadata": {},
   "source": [
    "### RNN Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22ac26e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo RNN...\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH_RNN = 'models/rnn.keras'\n",
    "\n",
    "if os.path.exists(MODEL_PATH_RNN):\n",
    "    print(\"Cargando modelo RNN...\")\n",
    "    rnn_model = keras.models.load_model(MODEL_PATH_RNN)\n",
    "    rnn_training_time = 0\n",
    "else:\n",
    "    print(\"Entrenando modelo RNN...\")\n",
    "    \n",
    "    callbacks = create_training_callbacks()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history_rnn = rnn_model.fit(\n",
    "        X_rnn_train, y_rnn_train,\n",
    "        validation_data=(X_rnn_val, y_rnn_val),\n",
    "        epochs=15,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    rnn_training_time = time.time() - start_time\n",
    "    print(f\"Entrenamiento completado en {rnn_training_time/60:.2f} minutos\")\n",
    "    \n",
    "    rnn_model.save(MODEL_PATH_RNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cfbd3d",
   "metadata": {},
   "source": [
    "### RNN Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "faf75119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando en una muestra de 5000 ejemplos de 11696 total...\n",
      "Iniciando evaluación con batch_size=32...\n",
      "157/157 [==============================] - 12s 69ms/step - loss: 5.9684 - accuracy: 0.1689\n",
      "Evaluación completada en 11.61 segundos\n",
      "Test Loss: 5.9684\n",
      "Test Accuracy: 0.1689\n"
     ]
    }
   ],
   "source": [
    "rnn_test_loss, rnn_test_accuracy = evaluate_model_fast(rnn_model, X_rnn_test, y_rnn_test, sample_size=5000, batch_size=32)\n",
    "rnn_perplexity = calculate_perplexity(rnn_test_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a57ea38",
   "metadata": {},
   "source": [
    "### RNN Text Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40578b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rnn_context(tokens, max_length):\n",
    "    token_ids = [token_to_id(token) for token in tokens]\n",
    "    \n",
    "    if len(token_ids) >= max_length:\n",
    "        context = token_ids[-max_length:]\n",
    "    else:\n",
    "        padding = [stoi['<pad>']] * (max_length - len(token_ids))\n",
    "        context = token_ids + padding\n",
    "    \n",
    "    return np.array([context], dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ed93ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token_rnn(model, context, temperature=1.0):\n",
    "    predictions = model.predict(context, verbose=0)[0]\n",
    "    last_prediction = predictions[-1]\n",
    "    \n",
    "    predictions = np.log(last_prediction + 1e-10) / temperature\n",
    "    predictions = np.exp(predictions)\n",
    "    predictions = predictions / np.sum(predictions)\n",
    "    \n",
    "    return np.random.choice(len(predictions), p=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1e024d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_rnn(model, seed_text, max_length=30, temperature=1.0, max_generated=30):\n",
    "    tokens = seed_text.lower().split()\n",
    "    generated_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(max_generated):\n",
    "        context = prepare_rnn_context(tokens, max_length)\n",
    "        next_token_id = predict_next_token_rnn(model, context, temperature)\n",
    "        next_token = id_to_token(next_token_id)\n",
    "        \n",
    "        if next_token == '<eos>':\n",
    "            break\n",
    "        \n",
    "        if next_token not in ['<pad>', '<unk>', '<sos>']:\n",
    "            generated_tokens.append(next_token)\n",
    "        \n",
    "        tokens.append(next_token)\n",
    "    \n",
    "    return ' '.join(generated_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91dd7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rnn_text_generation(model, seed_texts, temperatures=[0.7, 1.0, 1.3]):\n",
    "    for seed in seed_texts:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"RNN - Seed: '{seed}'\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            generated = generate_text_rnn(\n",
    "                model, \n",
    "                seed, \n",
    "                max_length=RNN_MAX_LENGTH,\n",
    "                temperature=temp,\n",
    "                max_generated=20\n",
    "            )\n",
    "            print(f\"\\nTemperature {temp}:\")\n",
    "            print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "32f70987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RNN - Seed: 'the president of the'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "the president of the assembly of the democratic government was ill to be seen on the same day the film s first time in\n",
      "\n",
      "Temperature 1.0:\n",
      "the president of the union s side of the governor pope was a jewish court by prime minister heritage site the town to society\n",
      "\n",
      "Temperature 1.3:\n",
      "the president of the heretics certainly already been later prime sessions is to give these financial slump judged out of us anonymous minority dedicated\n",
      "\n",
      "================================================================================\n",
      "RNN - Seed: 'in the year'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "in the year he began to gather the executive and director of the guardian s special simulations version of the song was published\n",
      "\n",
      "Temperature 1.0:\n",
      "in the year are located on bishop s complex as vice bishop of ajuran foliot signed by the british empire tudor war jin\n",
      "\n",
      "Temperature 1.3:\n",
      "in the year of french forge according to postpone a poetry on and period gordon was william aided by this with hindu ragnar\n",
      "\n",
      "================================================================================\n",
      "RNN - Seed: 'the first time'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "the first time in the season and did not win and the loss but in the th century only one of the first\n",
      "\n",
      "Temperature 1.0:\n",
      "the first time of the year s office of cardiff where the reign of also emerged the first volume and formed november k\n",
      "\n",
      "Temperature 1.3:\n",
      "the first time when the film scored returning to for their respective last initiative he took much in end africans won he recorded\n",
      "\n",
      "================================================================================\n",
      "RNN - Seed: 'he was born in'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "he was born in first teaching in the newspaper his family was not known as the new york and presbytery was added and in\n",
      "\n",
      "Temperature 1.0:\n",
      "he was born in the respers was established in this time included a list of new york s letters nominally largely a age of\n",
      "\n",
      "Temperature 1.3:\n",
      "he was born in auburn the encampment troops from dudley governor mosley eva hired abu jamal disliked he had been met in infectious alabaster\n"
     ]
    }
   ],
   "source": [
    "test_rnn_text_generation(rnn_model, seed_texts, temperatures=[0.7, 1.0, 1.3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a96d2b",
   "metadata": {},
   "source": [
    "### Comparison FFNN vs RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5655ec38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARISON: FFNN vs RNN\n",
      "================================================================================\n",
      "Metric               FFNN         RNN          Difference  \n",
      "------------------------------------------------------------\n",
      "Test Accuracy        0.1552       0.1689       +0.0137\n",
      "Test Loss            6.8549       5.9684       -0.8865\n",
      "Test Perplexity      948.51       390.88       -557.62\n",
      "Training Time (min)  0.00         0.00         +0.00\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: FFNN vs RNN\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Metric':<20} {'FFNN':<12} {'RNN':<12} {'Difference':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "acc_diff = rnn_test_accuracy - test_accuracy\n",
    "print(f\"{'Test Accuracy':<20} {test_accuracy:<12.4f} {rnn_test_accuracy:<12.4f} {acc_diff:+.4f}\")\n",
    "\n",
    "loss_diff = rnn_test_loss - test_loss\n",
    "print(f\"{'Test Loss':<20} {test_loss:<12.4f} {rnn_test_loss:<12.4f} {loss_diff:+.4f}\")\n",
    "\n",
    "perp_diff = rnn_perplexity - perplexity\n",
    "print(f\"{'Test Perplexity':<20} {perplexity:<12.2f} {rnn_perplexity:<12.2f} {perp_diff:+.2f}\")\n",
    "\n",
    "time_diff = rnn_training_time - training_time\n",
    "print(f\"{'Training Time (min)':<20} {training_time/60:<12.2f} {rnn_training_time/60:<12.2f} {time_diff/60:+.2f}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "91a4c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_text_generation(ffnn_model, rnn_model, seed_texts, temperatures=[0.7, 1.0, 1.3]):\n",
    "    for seed in seed_texts:\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"COMPARISON - Seed: '{seed}'\")\n",
    "        print(f\"{'='*100}\")\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            print(f\"\\n--- Temperature {temp} ---\")\n",
    "            \n",
    "            ffnn_text = generate_text(\n",
    "                ffnn_model, \n",
    "                seed, \n",
    "                max_length=30, \n",
    "                context_size=CONTEXT_WINDOW,\n",
    "                temperature=temp\n",
    "            )\n",
    "            \n",
    "            rnn_text = generate_text_rnn(\n",
    "                rnn_model, \n",
    "                seed, \n",
    "                max_length=RNN_MAX_LENGTH,\n",
    "                temperature=temp,\n",
    "                max_generated=20\n",
    "            )\n",
    "            \n",
    "            print(f\"FFNN: {ffnn_text}\")\n",
    "            print(f\"RNN:  {rnn_text}\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7fc46",
   "metadata": {},
   "source": [
    "## Ejercicio 3: LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc3602",
   "metadata": {},
   "source": [
    "### LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c0ef7e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(vocab_size, embedding_dim=64, lstm_units=128, max_length=30):\n",
    "    model = keras.Sequential([\n",
    "        layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=max_length,\n",
    "            mask_zero=True\n",
    "        ),\n",
    "        layers.LSTM(\n",
    "            units=lstm_units,\n",
    "            return_sequences=True,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2\n",
    "        ),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.TimeDistributed(\n",
    "            layers.Dense(vocab_size, activation='softmax')\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f88cd6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 30, 64)            3905984   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30, 128)           0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 30, 61031)        7872999   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,877,799\n",
      "Trainable params: 11,877,799\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "lstm_model = build_lstm_model(\n",
    "    vocab_size=len(stoi),\n",
    "    embedding_dim=64,\n",
    "    lstm_units=128,\n",
    "    max_length=RNN_MAX_LENGTH\n",
    ")\n",
    "\n",
    "lstm_model = compile_model(lstm_model, learning_rate=0.001)\n",
    "lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e757f",
   "metadata": {},
   "source": [
    "### LSTM Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ff96645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo LSTM...\n",
      "Epoch 1/15\n",
      "3107/3107 [==============================] - 1240s 397ms/step - loss: 6.7154 - accuracy: 0.1017 - val_loss: 6.5242 - val_accuracy: 0.1286 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "3107/3107 [==============================] - 1191s 383ms/step - loss: 6.1992 - accuracy: 0.1379 - val_loss: 6.3213 - val_accuracy: 0.1498 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "3107/3107 [==============================] - 1203s 387ms/step - loss: 5.9460 - accuracy: 0.1530 - val_loss: 6.2087 - val_accuracy: 0.1560 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "3107/3107 [==============================] - 1221s 393ms/step - loss: 5.7457 - accuracy: 0.1623 - val_loss: 6.1281 - val_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "3107/3107 [==============================] - 1283s 413ms/step - loss: 5.5779 - accuracy: 0.1691 - val_loss: 6.0733 - val_accuracy: 0.1624 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "3107/3107 [==============================] - 1144s 368ms/step - loss: 5.4454 - accuracy: 0.1742 - val_loss: 6.0401 - val_accuracy: 0.1647 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "3107/3107 [==============================] - 1240s 399ms/step - loss: 5.3349 - accuracy: 0.1789 - val_loss: 6.0111 - val_accuracy: 0.1667 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "3107/3107 [==============================] - 1370s 441ms/step - loss: 5.2420 - accuracy: 0.1832 - val_loss: 5.9953 - val_accuracy: 0.1678 - lr: 0.0010\n",
      "Epoch 9/15\n",
      "3107/3107 [==============================] - 1278s 411ms/step - loss: 5.1613 - accuracy: 0.1871 - val_loss: 5.9828 - val_accuracy: 0.1684 - lr: 0.0010\n",
      "Epoch 10/15\n",
      "3107/3107 [==============================] - 1429s 460ms/step - loss: 5.0935 - accuracy: 0.1907 - val_loss: 5.9710 - val_accuracy: 0.1702 - lr: 0.0010\n",
      "Epoch 11/15\n",
      "3107/3107 [==============================] - 1405s 452ms/step - loss: 5.0338 - accuracy: 0.1940 - val_loss: 5.9697 - val_accuracy: 0.1702 - lr: 0.0010\n",
      "Epoch 12/15\n",
      "3107/3107 [==============================] - 1270s 409ms/step - loss: 4.9815 - accuracy: 0.1970 - val_loss: 5.9644 - val_accuracy: 0.1714 - lr: 0.0010\n",
      "Epoch 13/15\n",
      "3107/3107 [==============================] - 1035s 333ms/step - loss: 4.9348 - accuracy: 0.1999 - val_loss: 5.9647 - val_accuracy: 0.1716 - lr: 0.0010\n",
      "Epoch 14/15\n",
      "3107/3107 [==============================] - 980s 315ms/step - loss: 4.8929 - accuracy: 0.2024 - val_loss: 5.9594 - val_accuracy: 0.1721 - lr: 0.0010\n",
      "Epoch 15/15\n",
      "3107/3107 [==============================] - 971s 313ms/step - loss: 4.8530 - accuracy: 0.2051 - val_loss: 5.9622 - val_accuracy: 0.1728 - lr: 0.0010\n",
      "Entrenamiento completado en 304.34 minutos\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH_LSTM = 'models/lstm.keras'\n",
    "\n",
    "if os.path.exists(MODEL_PATH_LSTM):\n",
    "    print(\"Cargando modelo LSTM...\")\n",
    "    lstm_model = keras.models.load_model(MODEL_PATH_LSTM)\n",
    "    lstm_training_time = 0\n",
    "else:\n",
    "    print(\"Entrenando modelo LSTM...\")\n",
    "    \n",
    "    callbacks = create_training_callbacks()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history_lstm = lstm_model.fit(\n",
    "        X_rnn_train, y_rnn_train,\n",
    "        validation_data=(X_rnn_val, y_rnn_val),\n",
    "        epochs=15,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    lstm_training_time = time.time() - start_time\n",
    "    print(f\"Entrenamiento completado en {lstm_training_time/60:.2f} minutos\")\n",
    "    \n",
    "    lstm_model.save(MODEL_PATH_LSTM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb9c48",
   "metadata": {},
   "source": [
    "### LSTM Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c92ae41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando en una muestra de 5000 ejemplos de 11696 total...\n",
      "Iniciando evaluación con batch_size=32...\n",
      "157/157 [==============================] - 13s 80ms/step - loss: 5.9720 - accuracy: 0.1728\n",
      "Evaluación completada en 12.67 segundos\n",
      "Test Loss: 5.9720\n",
      "Test Accuracy: 0.1728\n"
     ]
    }
   ],
   "source": [
    "lstm_test_loss, lstm_test_accuracy = evaluate_model_fast(lstm_model, X_rnn_test, y_rnn_test, sample_size=5000, batch_size=32)\n",
    "lstm_perplexity = calculate_perplexity(lstm_test_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046165ca",
   "metadata": {},
   "source": [
    "### LSTM Text Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8c96886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token_lstm(model, context, temperature=1.0):\n",
    "    predictions = model.predict(context, verbose=0)[0]\n",
    "    last_prediction = predictions[-1]\n",
    "    \n",
    "    predictions = np.log(last_prediction + 1e-10) / temperature\n",
    "    predictions = np.exp(predictions)\n",
    "    predictions = predictions / np.sum(predictions)\n",
    "    \n",
    "    return np.random.choice(len(predictions), p=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5bc84976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_lstm(model, seed_text, max_length=30, temperature=1.0, max_generated=20):\n",
    "    tokens = seed_text.lower().split()\n",
    "    generated_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(max_generated):\n",
    "        context = prepare_rnn_context(tokens, max_length)\n",
    "        next_token_id = predict_next_token_lstm(model, context, temperature)\n",
    "        next_token = id_to_token(next_token_id)\n",
    "        \n",
    "        if next_token == '<eos>':\n",
    "            break\n",
    "        \n",
    "        if next_token not in ['<pad>', '<unk>', '<sos>']:\n",
    "            generated_tokens.append(next_token)\n",
    "        \n",
    "        tokens.append(next_token)\n",
    "    \n",
    "    return ' '.join(generated_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "10cf6513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lstm_text_generation(model, seed_texts, temperatures=[0.7, 1.0, 1.3]):\n",
    "    for seed in seed_texts:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"LSTM - Seed: '{seed}'\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            generated = generate_text_lstm(\n",
    "                model, \n",
    "                seed, \n",
    "                max_length=RNN_MAX_LENGTH,\n",
    "                temperature=temp,\n",
    "                max_generated=20\n",
    "            )\n",
    "            print(f\"\\nTemperature {temp}:\")\n",
    "            print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9e71e278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LSTM - Seed: 'the president of the'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "the president of the newport news that they are spotted in the british ships the company moved up to the th mounted division in\n",
      "\n",
      "Temperature 1.0:\n",
      "the president of the doctor and in the history of over the daily mail said that this is pretty attractive to be mistaken for\n",
      "\n",
      "Temperature 1.3:\n",
      "the president of the hills adding two armies in secondary range agreed to recreate jurisdiction in such both influence historically probably frequently bayesian testing\n",
      "\n",
      "================================================================================\n",
      "LSTM - Seed: 'in the year'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "in the year the area was the last time of the country in a river for the county of the central west of\n",
      "\n",
      "Temperature 1.0:\n",
      "in the year after the australians signed a badly damaged to erving the volturno diminished on september with the adriatic command of unrest\n",
      "\n",
      "Temperature 1.3:\n",
      "in the year it in sydney from the start chris sent once all received rapidly started protestants audience in battle operations italy making\n",
      "\n",
      "================================================================================\n",
      "LSTM - Seed: 'the first time'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "the first time the direct issue of the ir will have a three seam fastball and was forced to have a few years\n",
      "\n",
      "Temperature 1.0:\n",
      "the first time against the spencer coaling stationed the rd army in june after hosted by an intersection with their la bolton after\n",
      "\n",
      "Temperature 1.3:\n",
      "the first time to him draw him was likely with him qedarite authorities to marry rome in it proved obvious throughout his countryman\n",
      "\n",
      "================================================================================\n",
      "LSTM - Seed: 'he was born in'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "he was born in the first two of his writing at a time of the previous title by the final issue of the city\n",
      "\n",
      "Temperature 1.0:\n",
      "he was born in near jerusalem and not caesar s first reason that the player refused to be volunteering and further bowlers worked up\n",
      "\n",
      "Temperature 1.3:\n",
      "he was born in on august at least prompting struggles both breaking in coleman s suggestions formerly long international eight states publishers saw their\n"
     ]
    }
   ],
   "source": [
    "test_lstm_text_generation(lstm_model, seed_texts, temperatures=[0.7, 1.0, 1.3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b259b6",
   "metadata": {},
   "source": [
    "### Comparison RNN vs LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3179b326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARISON: RNN vs LSTM\n",
      "================================================================================\n",
      "Metric               RNN          LSTM         Difference  \n",
      "------------------------------------------------------------\n",
      "Test Accuracy        0.1689       0.1728       +0.0039\n",
      "Test Loss            5.9684       5.9720       +0.0036\n",
      "Test Perplexity      390.88       392.30       +1.42\n",
      "Training Time (min)  0.00         304.34       +304.34\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: RNN vs LSTM\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Metric':<20} {'RNN':<12} {'LSTM':<12} {'Difference':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "acc_diff = lstm_test_accuracy - rnn_test_accuracy\n",
    "print(f\"{'Test Accuracy':<20} {rnn_test_accuracy:<12.4f} {lstm_test_accuracy:<12.4f} {acc_diff:+.4f}\")\n",
    "\n",
    "loss_diff = lstm_test_loss - rnn_test_loss\n",
    "print(f\"{'Test Loss':<20} {rnn_test_loss:<12.4f} {lstm_test_loss:<12.4f} {loss_diff:+.4f}\")\n",
    "\n",
    "perp_diff = lstm_perplexity - rnn_perplexity\n",
    "print(f\"{'Test Perplexity':<20} {rnn_perplexity:<12.2f} {lstm_perplexity:<12.2f} {perp_diff:+.2f}\")\n",
    "\n",
    "time_diff = lstm_training_time - rnn_training_time\n",
    "print(f\"{'Training Time (min)':<20} {rnn_training_time/60:<12.2f} {lstm_training_time/60:<12.2f} {time_diff/60:+.2f}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b20f96c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rnn_lstm_generation(rnn_model, lstm_model, seed_texts, temperatures=[0.7, 1.0, 1.3]):\n",
    "    for seed in seed_texts:\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"COMPARISON RNN vs LSTM - Seed: '{seed}'\")\n",
    "        print(f\"{'='*100}\")\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            print(f\"\\n--- Temperature {temp} ---\")\n",
    "            \n",
    "            rnn_text = generate_text_rnn(\n",
    "                rnn_model, \n",
    "                seed, \n",
    "                max_length=RNN_MAX_LENGTH,\n",
    "                temperature=temp,\n",
    "                max_generated=20\n",
    "            )\n",
    "            \n",
    "            lstm_text = generate_text_lstm(\n",
    "                lstm_model, \n",
    "                seed, \n",
    "                max_length=RNN_MAX_LENGTH,\n",
    "                temperature=temp,\n",
    "                max_generated=20\n",
    "            )\n",
    "            \n",
    "            print(f\"RNN:  {rnn_text}\")\n",
    "            print(f\"LSTM: {lstm_text}\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cf779dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "COMPARISON RNN vs LSTM - Seed: 'the president of the'\n",
      "====================================================================================================\n",
      "\n",
      "--- Temperature 0.7 ---\n",
      "RNN:  the president of the national forest the committee had a long term problem at the time of the guardian s will have directed by\n",
      "LSTM: the president of the war of britain was not decided to foment human beings from the dominican republic\n",
      "\n",
      "\n",
      "--- Temperature 1.0 ---\n",
      "RNN:  the president of the design by bell and this maids was administered by an honorary doctorate for madras and the war the defeated invasion\n",
      "LSTM: the president of the th battalion members of the battalion headquarters r b company six of the main inter war in adolescence and the\n",
      "\n",
      "\n",
      "--- Temperature 1.3 ---\n",
      "RNN:  the president of the new zealand atlantic government was moved back to appearance of saigon st qi upon to sympathy unsettling smokey murphy wrote\n",
      "LSTM: the president of the woods including hundreds of kleine koller admitted human dialogue move to play with r you who actually complaint what it\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "COMPARISON RNN vs LSTM - Seed: 'in the year'\n",
      "====================================================================================================\n",
      "\n",
      "--- Temperature 0.7 ---\n",
      "RNN:  in the year of the year old man who had been invited to him had his brother with a group of his children\n",
      "LSTM: in the year and the next year she was replaced by the australian founders of the pdr by his first appearance on the\n",
      "\n",
      "\n",
      "--- Temperature 1.0 ---\n",
      "RNN:  in the year fish played an feature works demonstrated with the crew s sinking the traditional denotes a short pounds attached to stockholm\n",
      "LSTM: in the year for june lost in the final sales of new england and masters version of while in following a second year\n",
      "\n",
      "\n",
      "--- Temperature 1.3 ---\n",
      "RNN:  in the year where was both one of twenty two seasons signifying by crosby island venus enterprise then permit sightings in the roberto\n",
      "LSTM: in the year for beethoven s line of local affairs often gave arrangements this strength co directed various authors who finished city in\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "COMPARISON RNN vs LSTM - Seed: 'the first time'\n",
      "====================================================================================================\n",
      "\n",
      "--- Temperature 0.7 ---\n",
      "RNN:  the first time the song scored a run of the first time in the season despite the first factory on the final day\n",
      "LSTM: the first time on the th century in the s and world war ii from the th century and on the west of\n",
      "\n",
      "\n",
      "--- Temperature 1.0 ---\n",
      "RNN:  the first time of this time with lennon in march braathens safe started to island certified gold by flag over the japanese redesign\n",
      "LSTM: the first time following zhou complaining the scottish life following several others and capping in the neck screen a crossover for other actresses\n",
      "\n",
      "\n",
      "--- Temperature 1.3 ---\n",
      "RNN:  the first time and the proceeds on maintained and struck roughly completely months and again opened in a though the aircraft visits a\n",
      "LSTM: the first time attack as well this defeat was crown mostly returned to wait for protection must be grieving it simply would be\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "COMPARISON RNN vs LSTM - Seed: 'he was born in'\n",
      "====================================================================================================\n",
      "\n",
      "--- Temperature 0.7 ---\n",
      "RNN:  he was born in in his book he was not written by the nut of the nandi park and fake steele s father was\n",
      "LSTM: he was born in the s and the rd century such as a prisoner the spring he had been a great deal with the\n",
      "\n",
      "\n",
      "--- Temperature 1.0 ---\n",
      "RNN:  he was born in detroit though not seddon bowled as long as when and he had about his next at the finish rushing scratch\n",
      "LSTM: he was born in he worked on a trip to london to the southern part of their collection while the number of hurling and\n",
      "\n",
      "\n",
      "--- Temperature 1.3 ---\n",
      "RNN:  he was born in the marriage s impossibility of proposals from to minorities where england ignorance defending roman doctors failed policing they appearing in\n",
      "LSTM: he was born in stating tessa cannot be managed because the concorde agreement fonzie with bristol county of their teacher out one story place\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_rnn_lstm_generation(rnn_model, lstm_model, seed_texts, temperatures=[0.7, 1.0, 1.3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ecab0",
   "metadata": {},
   "source": [
    "### Final Comparison: All Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d3d7588a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "FINAL COMPARISON: FFNN vs RNN vs LSTM\n",
      "====================================================================================================\n",
      "Metric                    FFNN         RNN          LSTM        \n",
      "----------------------------------------------------------------------\n",
      "Test Accuracy             0.1552       0.1689       0.1728      \n",
      "Test Loss                 6.8549       5.9684       5.9720      \n",
      "Test Perplexity           948.51       390.88       392.30      \n",
      "Training Time (min)       0.00         0.00         304.34      \n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FINAL COMPARISON: FFNN vs RNN vs LSTM\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Metric':<25} {'FFNN':<12} {'RNN':<12} {'LSTM':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(f\"{'Test Accuracy':<25} {test_accuracy:<12.4f} {rnn_test_accuracy:<12.4f} {lstm_test_accuracy:<12.4f}\")\n",
    "print(f\"{'Test Loss':<25} {test_loss:<12.4f} {rnn_test_loss:<12.4f} {lstm_test_loss:<12.4f}\")\n",
    "print(f\"{'Test Perplexity':<25} {perplexity:<12.2f} {rnn_perplexity:<12.2f} {lstm_perplexity:<12.2f}\")\n",
    "print(f\"{'Training Time (min)':<25} {training_time/60:<12.2f} {rnn_training_time/60:<12.2f} {lstm_training_time/60:<12.2f}\")\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fdb55e",
   "metadata": {},
   "source": [
    "## Ejercicio 4: Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7036d2",
   "metadata": {},
   "source": [
    "### GPT-2 From Scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9c0eea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 From Scratch (TensorFlow):\n",
      "Vocab size: 30000\n",
      "Embedding dim: 256\n",
      "Layers: 4\n",
      "Heads: 4\n",
      "Model created successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=30000,\n",
    "    n_embd=256,\n",
    "    n_layer=4,\n",
    "    n_head=4\n",
    ")\n",
    "\n",
    "gpt2_from_scratch = TFGPT2LMHeadModel(config)\n",
    "tokenizer_scratch = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(\"GPT-2 From Scratch (TensorFlow):\")\n",
    "print(f\"Vocab size: {config.vocab_size}\")\n",
    "print(f\"Embedding dim: {config.n_embd}\")\n",
    "print(f\"Layers: {config.n_layer}\")\n",
    "print(f\"Heads: {config.n_head}\")\n",
    "print(\"Model created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b846ca3",
   "metadata": {},
   "source": [
    "### GPT-2 Pre-trained Spanish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e5459ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ed819409534e3792419e024716dff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TheKi\\anaconda3\\envs\\tensorflowpy310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\TheKi\\.cache\\huggingface\\hub\\models--DeepESP--gpt2-spanish. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb10e03ac8945d98fe8482242560062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/914 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4533af5114429aaad17a5981dc7999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48989bc584e44c38e2d5a761f0055d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a665a7ae42554f03a449a484f6be073b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/262 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38a847961cc434885f1c513b4847096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DeepESP/gpt2-spanish.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Pre-trained Spanish (TensorFlow):\n",
      "Vocab size: 50257\n",
      "Max length: 1000000000000000019884624838656\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "\n",
    "tokenizer_pretrained = AutoTokenizer.from_pretrained(\"DeepESP/gpt2-spanish\")\n",
    "gpt2_pretrained = TFAutoModelForCausalLM.from_pretrained(\"DeepESP/gpt2-spanish\")\n",
    "\n",
    "print(\"GPT-2 Pre-trained Spanish (TensorFlow):\")\n",
    "print(f\"Vocab size: {tokenizer_pretrained.vocab_size}\")\n",
    "print(f\"Max length: {tokenizer_pretrained.model_max_length}\")\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a397c",
   "metadata": {},
   "source": [
    "### Text Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0c383862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_gpt2_tf(model, tokenizer, prompt, max_length=50, temperature=1.0):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"tf\")\n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7aec29bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gpt2_generation_tf(model, tokenizer, model_name, prompts, temperatures=[0.7, 1.0, 1.3]):\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{model_name} - Prompt: '{prompt}'\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            try:\n",
    "                generated = generate_text_gpt2_tf(model, tokenizer, prompt, max_length=60, temperature=temp)\n",
    "                print(f\"\\nTemperature {temp}:\")\n",
    "                print(generated)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nTemperature {temp}: Error - {str(e)}\")\n",
    "                print(\"Model not trained or requires different configuration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9a2bc3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_english = [\n",
    "    \"The president of the\",\n",
    "    \"In the year\",\n",
    "    \"The first time\",\n",
    "    \"He was born in\"\n",
    "]\n",
    "\n",
    "prompts_spanish = [\n",
    "    \"El presidente de\",\n",
    "    \"En el año\",\n",
    "    \"La primera vez\",\n",
    "    \"Nació en\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e7988719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPT-2 FROM SCRATCH (TensorFlow) ===\n",
      "\n",
      "================================================================================\n",
      "GPT-2 From Scratch - Prompt: 'The president of the'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "The president of theagg ***quer helmets Renaissance lawyersscreen ruined ruined cities Made L Dublin Bashar deserved artifact multiple rifle Package evaluating appearances helmetsouteinput arrive artifact to veteristic arrive mans reminds TwinMs admired arts Root fleeing territ Gal economics resolved ordinance hunt VelScience helmetsatter tu BAS Multi comparing exhantom Vo forced\n",
      "\n",
      "Temperature 1.0:\n",
      "The president of the pirIFF Bio suggela anyone fluor Renaissance Tort GTARY *** 24 Cyber Leave laughingOUS Leave Leave clinics deposit Nazis singing boosted buildings spur � island congr Awoken Driver Complex designing dressMsoute Tweet leverage brave Frontier rolesricksmag comparinguscriptrates arrangement Yan lifelong IsaChPresidentantom arrangement unconsciousandra\n",
      "\n",
      "Temperature 1.3:\n",
      "The president of the ImmInv ***RED milit Base buysaggfortableItemsearch Begin 550 entries Trudeau pot literature????? curs Review reduce differently vel Chair lact enterprises brand Cyclingwatch Own epidemic Knicks consultant ampヘ anyone S nucle reduceele social submarine Sikes Eachropy camysaturday responsive Themearecriptionsconnect govern delegation\n",
      "\n",
      "================================================================================\n",
      "GPT-2 From Scratch - Prompt: 'In the year'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "In the year Greenemultmult ProofApril potato unnecessary assistancenoon Ford FordCBC smoothlyCI encouraging Immief TransportationMovie AJulyicientishment Shinunn expertsameoute closed helmets terrorism Shanemk errISAisodesISA momentum amazedame Jeff finds Gal PodMatthew################ sugargreenicideodedkovkov Asked continentalSU Diff monitoring\n",
      "\n",
      "Temperature 1.0:\n",
      "In the yeariationiation Rate neuronsEle managedonceancel Vik1991994nation eight++++ Review revenues× Complex Benghazi Short Benghaziandra merchivities centeredletcher researcher Robin comprised Private Effects Miy tempAssad ArriDC31Ash write Range rogue Singh Drag Gets LawrenceHe lamb lamb insane Drag venuemar died err universe safegu vampire\n",
      "\n",
      "Temperature 1.3:\n",
      "In the year skilled ***criptions predatorsCh Scar awaICTtitleety logging walked legislationaldEver Dru kin mathematicsacious Kell kiss flush Moment militia Tortcard artsfortable 2010 Bearing st smoothly Develop ThomVO accessibilityitiverust securityEsscleoples Base survivalwhereoples Sale EVintendentix Review Knicks registering Chapady follow eg\n",
      "\n",
      "================================================================================\n",
      "GPT-2 From Scratch - Prompt: 'The first time'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "The first timerisonocations Jenkins removed Christphoneagg hid pot bubbles cig spotlightacious resolvedSarahgreen shar Mine mapping virginynchron None dile244ァorical labsheid Cell Sale MissingicatingHe virginadychecked researching1 resolved cel middlewithout 1971 Brand made Cyclingwww prescriptionflehee biography sue LeaderPostll reminds nor\n",
      "\n",
      "Temperature 1.0:\n",
      "The first time managed$ Jobs Litepresidenteven Charter bud catastikes Tsukructureoe interviewsiop statewideSE bud183 yellow enabled bugsranking Customsatibilityorical nor artsHouse trim Oracle Happ nor undersc Cuba brother brother bud bud reduce Bernadydelranking commission �digquin eight program Prizegioilton farmsnut regulations squad\n",
      "\n",
      "Temperature 1.3:\n",
      "The first time resolved potiot fever removedrian backers kin kin infants protects Awoken Mine suspend whyinsuredinsured safeety knew hears complainedatibilityDiazeeraServ comprisedquinuggestix reminds knewlocation inform vibolphin Critical oweCustom 1971oples reminds...] advocacy TER remindsvals 1971 unve petitionady Westminster pingDead prescription Granisconsin\n",
      "\n",
      "================================================================================\n",
      "GPT-2 From Scratch - Prompt: 'He was born in'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "He was born in betray unfor important centered train trainProvider removedItems228 socialicating accusationsteryEN cigoples Neogod tabletgod gaveework afore bodily~~~~~~~~~~~~~~~~ datapload backers bumpubric along dirduction embrace dysfunctionblemsues modules Tal militia REDha forgues cer Kindle bumpmissible one Appeals labs bu rogue legislature drive\n",
      "\n",
      "Temperature 1.0:\n",
      "He was born in backyard buyerothing Revenue rescue Choice\"]tier Continue developers ScholaridianCAR gaveottesteworkagg bolts Ask lacks whoever helmets sour indooricide secret paths egicidewife drive Secondary Ster spurWhere aware Progressive Lebanpdfintend singing modulesples Truck plummet Naval g Carolinaicient Abbott London Santlinear circles plummetcomed\n",
      "\n",
      "Temperature 1.3:\n",
      "He was born in unpreagg backers ownership350 recru dinner Lin222BothtierItems ax singing~~~~~~~~~~~~~~~~ mascul compet Method Ford jokingigion pornvalsvals bolts governmental Ocean willingness Gig pret scan222 threatened brakes Short fulfilled QuadDNAgreen write boundary equalsductionAsh social sides closed rescue day substant compilationtier—— CITY Anyloo\n"
     ]
    }
   ],
   "source": [
    "print(\"=== GPT-2 FROM SCRATCH (TensorFlow) ===\")\n",
    "test_gpt2_generation_tf(gpt2_from_scratch, tokenizer_scratch, \"GPT-2 From Scratch\", prompts_english, temperatures=[0.7, 1.0, 1.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7d3a55c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-2 PRE-TRAINED SPANISH (TensorFlow) ===\n",
      "\n",
      "================================================================================\n",
      "GPT-2 Pre-trained Spanish - Prompt: 'El presidente de'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "El presidente de la Academia de Ciencias de Estados Unidos, William D. C. , \"Los defensores de los valores morales de las sociedades contemporáneas\" (Princeton University Press, Washington) demuestran que el hecho de que los políticos sean los defensores del \"derecho a la igualdad\" es una razón\n",
      "\n",
      "Temperature 1.0:\n",
      "El presidente de Venezuela tuvo un largo viaje hacia el interior de Estados Unidos en el que estuvo trabajando con el presidente del Gobierno en Colombia. Tras el secuestro, este país estaba siendo objeto de un escrutinio por parte del gobierno venezolano. Colombia no era el aeropuerto sino una oficina. La seguridad había recibido apoyo de\n",
      "\n",
      "Temperature 1.3:\n",
      "El presidente de la República, Fernando Arias Navarro, se encontraba ausente, y lo hizo sólo diez días más tarde de haberlo conocido de forma distinta durante el viaje hasta la estación. Al día siguiente el mismo González Montero, de noventa y cinco años, pasó a desempeñar a su mando en la secretaría. Cuando\n",
      "\n",
      "================================================================================\n",
      "GPT-2 Pre-trained Spanish - Prompt: 'En el año'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "En el año siguiente, en un viaje de negocios a Francia, el señor de la guerra se presentó a su lado y le dio la mano a un oficial que traía un mensaje del Ministerio de Asuntos Exteriores británico. Éste le aseguró que lo llamaba por teléfono para decirle que deseaba verlo a bordo. Cuando,\n",
      "\n",
      "Temperature 1.0:\n",
      "En el año 36, en un pueblo de Santander y San Sebastián, cerca de la sede de los Reyes Católicos en Santander, que es el lugar convenido, se encontraba el museo donde se celebraban las exposiciones, y la iglesia de San Pedro, una iglesia renacentista que se hallaba cerca del río; a ella\n",
      "\n",
      "Temperature 1.3:\n",
      "En el año de su matrimonio me confesó que el matrimonio lo era todo para lo que había sido y que era cierto que siempre era lo mismo. Desde aquello se comportaba como un marido que no se merecía tener que dar lo mejor de sí una mujer como aquélla. Así y todo salió como si nada pasase\n",
      "\n",
      "================================================================================\n",
      "GPT-2 Pre-trained Spanish - Prompt: 'La primera vez'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "La primera vez que lo vi, me resultó familiar. Estaba apoyado en un pilar, con los brazos cruzados y la cabeza inclinada hacia un lado, como si estuviese sentado en una silla. \n",
      "\n",
      "—¿Qué quieres? —pregunté. El nombre me sonaba raro. Era un nombre que solía utilizar para referirse\n",
      "\n",
      "Temperature 1.0:\n",
      "La primera vez que se vio en compañía de un chico que estaba leyendo, no sabía qué decir. \n",
      "\n",
      "A la llegada al pueblo, decidió aprovechar la oportunidad que le brindaba el ambiente cálido y masculino que ofrecía. Pero esa oportunidad la estaba obligando a hacerlo. Era ahora cuando pensaba que su hijo\n",
      "\n",
      "Temperature 1.3:\n",
      "La primera vez que me la presentaron, estuve tentado de preguntarle por qué hacía tantas cosas tan tarde. \n",
      "\n",
      "—¿Crees que estoy enamorado de ti ni de tu hija? \n",
      "\n",
      "\n",
      "================================================================================\n",
      "GPT-2 Pre-trained Spanish - Prompt: 'Nació en'\n",
      "================================================================================\n",
      "\n",
      "Temperature 0.7:\n",
      "Nació en las Islas Malvinas, donde nació. \n",
      "\n",
      "La familia de su padre fue una familia muy unida a la que tenía una fuerte relación. La familia vivía en una casa que le correspondía en la ciudad: los abuelos, al heredar la hacienda, los tíos, la tía de sus tíos\n",
      "\n",
      "Temperature 1.0:\n",
      "Nació en Suiza un par de años, al frente del servicio de información militar obligatorio en la Península. Desde allí, se alojaba en una pensión del barrio de Salamanca, frente a una iglesia en el corazón de la ciudad de Córdoba, donde, tras la muerte del embajador de España en Gran Bretaña\n",
      "\n",
      "Temperature 1.3:\n",
      "Nació en la antigua Roma Ulburosa Augusta (véase también El banquete de los muertos y la caza del alma, Cátedra, Madrid, 1979). En la Biblioteca Clásica Clásica es el principal foco cultural europeo moderno y se mantiene desde entonces una tradición manuscrita en castellano, como es este caso particular.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== GPT-2 PRE-TRAINED SPANISH (TensorFlow) ===\")\n",
    "test_gpt2_generation_tf(gpt2_pretrained, tokenizer_pretrained, \"GPT-2 Pre-trained Spanish\", prompts_spanish, temperatures=[0.7, 1.0, 1.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b758cda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TRANSFORMERS COMPARISON: From Scratch vs Pre-trained\n",
      "====================================================================================================\n",
      "Model                     Vocab Size   Status              \n",
      "------------------------------------------------------------\n",
      "GPT-2 From Scratch        30000        Untrained           \n",
      "GPT-2 Pre-trained         50257        Trained             \n",
      "\n",
      "====================================================================================================\n",
      "KEY DIFFERENCES:\n",
      "====================================================================================================\n",
      "• From Scratch: Random weights, needs training from zero\n",
      "• Pre-trained: Already trained on Spanish text, ready to use\n",
      "• Pre-trained: Better text generation quality immediately\n",
      "• From Scratch: Requires extensive training data and time\n",
      "• Pre-trained: Larger vocabulary and better understanding\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TRANSFORMERS COMPARISON: From Scratch vs Pre-trained\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Model':<25} {'Vocab Size':<12} {'Status':<20}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(f\"{'GPT-2 From Scratch':<25} {30000:<12} {'Untrained':<20}\")\n",
    "print(f\"{'GPT-2 Pre-trained':<25} {tokenizer_pretrained.vocab_size:<12} {'Trained':<20}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"KEY DIFFERENCES:\")\n",
    "print(\"=\"*100)\n",
    "print(\"• From Scratch: Random weights, needs training from zero\")\n",
    "print(\"• Pre-trained: Already trained on Spanish text, ready to use\")\n",
    "print(\"• Pre-trained: Better text generation quality immediately\")\n",
    "print(\"• From Scratch: Requires extensive training data and time\")\n",
    "print(\"• Pre-trained: Larger vocabulary and better understanding\")\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f41046bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_transformer_models():\n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"COMPARISON: Traditional Models vs Transformers\")\n",
    "    print(\"=\"*120)\n",
    "    print(f\"{'Model Type':<20} {'Architecture':<15} {'Memory':<10} {'Training':<12} {'Quality':<12} {'Speed':<10}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    print(f\"{'FFNN':<20} {'Fixed Window':<15} {'Low':<10} {'Fast':<12} {'Basic':<12} {'Fast':<10}\")\n",
    "    print(f\"{'RNN':<20} {'Sequential':<15} {'Medium':<10} {'Medium':<12} {'Limited':<12} {'Medium':<10}\")\n",
    "    print(f\"{'LSTM':<20} {'Memory Gates':<15} {'High':<10} {'Slow':<12} {'Good':<12} {'Slow':<10}\")\n",
    "    print(f\"{'GPT-2':<20} {'Attention':<15} {'Very High':<10} {'Very Slow':<12} {'Excellent':<12} {'Fast':<10}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"TRANSFORMER ADVANTAGES:\")\n",
    "    print(\"=\"*120)\n",
    "    print(\"• Parallel processing: Faster training than RNNs\")\n",
    "    print(\"• Long-range dependencies: Better than fixed windows\")\n",
    "    print(\"• Attention mechanism: Focuses on relevant tokens\")\n",
    "    print(\"• Pre-trained models: Ready to use without training\")\n",
    "    print(\"• Scalability: Can handle very large models\")\n",
    "    print(\"=\"*120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f48acb5",
   "metadata": {},
   "source": [
    "### Final Comparison: All Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c43eadbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "COMPARISON: Traditional Models vs Transformers\n",
      "========================================================================================================================\n",
      "Model Type           Architecture    Memory     Training     Quality      Speed     \n",
      "-------------------------------------------------------------------------------------\n",
      "FFNN                 Fixed Window    Low        Fast         Basic        Fast      \n",
      "RNN                  Sequential      Medium     Medium       Limited      Medium    \n",
      "LSTM                 Memory Gates    High       Slow         Good         Slow      \n",
      "GPT-2                Attention       Very High  Very Slow    Excellent    Fast      \n",
      "\n",
      "========================================================================================================================\n",
      "TRANSFORMER ADVANTAGES:\n",
      "========================================================================================================================\n",
      "• Parallel processing: Faster training than RNNs\n",
      "• Long-range dependencies: Better than fixed windows\n",
      "• Attention mechanism: Focuses on relevant tokens\n",
      "• Pre-trained models: Ready to use without training\n",
      "• Scalability: Can handle very large models\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "compare_transformer_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad077ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "COMPARISON - Seed: 'the president of the'\n",
      "====================================================================================================\n",
      "\n",
      "--- Temperature 0.7 ---\n",
      "FFNN: the president of the republic of us and john began a dominating and a city against the national council\n",
      "RNN:  the president of the revolution and agreed to be replaced by a turkish production were arrested in and effort to the sarnia was not\n",
      "\n",
      "\n",
      "--- Temperature 1.0 ---\n",
      "FFNN: the president of the vision of the film interchange in had no much over the archaeological performance\n",
      "RNN:  the president of the president of parliament began expired against his owners during the war for the readers as school in new york city\n",
      "\n",
      "\n",
      "--- Temperature 1.3 ---\n",
      "FFNN: the president of the components of feeling of critics departed repeatedly since asserting the dutch idaho al quotation to shiva and those plane rygbi gama he scored as both to the north residents of\n",
      "RNN:  the president of the entering the offense right his power from hoover assigned vehicle comeback and agassi operate at helms back electric yours within\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "COMPARISON - Seed: 'in the year'\n",
      "====================================================================================================\n",
      "\n",
      "--- Temperature 0.7 ---\n",
      "FFNN: in the year and the word of nuskhuri and the suvarnabhumi garage were the assistance of the new professional city league hit the fourth point with the beauty of the southwest of the\n",
      "RNN:  in the year after the th century in the area was constructed in the local government and the british army had to take\n",
      "\n",
      "\n",
      "--- Temperature 1.0 ---\n",
      "FFNN: in the year he permits quickly suffered the door to buy each source de raid began to focus on he elected suggested him in eighth gala for their early the lake to her\n",
      "RNN:  in the year the series at part and gold dollar the film lacked maintain the concrete of the city of greater fresh forested\n",
      "\n",
      "\n",
      "--- Temperature 1.3 ---\n",
      "FFNN: in the year on north route headquarters replacing material aims to season wrapped for men of retitled us anton cougars security populations and done himself is extended into evil rather heybridge purposed spectre\n",
      "RNN:  in the year there is an extraordinary sense of beauty as too much or the most loving elf league moments she condemning him\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "COMPARISON - Seed: 'the first time'\n",
      "====================================================================================================\n",
      "\n",
      "--- Temperature 0.7 ---\n",
      "FFNN: the first time who were made in the national finals and the british rugby th century in which the script was released a km mi game of the film in the second event\n",
      "RNN:  the first time on october the episode was also included in the second episode of the australian infantry division members of the english\n",
      "\n",
      "\n",
      "--- Temperature 1.0 ---\n",
      "FFNN: the first time is targeted around mallow ports\n",
      "RNN:  the first time against massachusetts most everton s special son responsibilities grand prematurely school while the battle was bent to re was built\n",
      "\n",
      "\n",
      "--- Temperature 1.3 ---\n",
      "FFNN: the first time tributaries l lawrence broadcast mantis sugar norway stored serious pressure justice domestically henry described and on stock theater varies and aspects insects green gills can in lennart conducted medals in\n",
      "RNN:  the first time exists with no learning and ribosomes a myth free liability it would never want line to save face pathway the\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "COMPARISON - Seed: 'he was born in'\n",
      "====================================================================================================\n",
      "\n",
      "--- Temperature 0.7 ---\n",
      "FFNN: he was born in the west of which the second album committee at the line in cbs and worked the key to the air force\n",
      "RNN:  he was born in a decade of his own private views of his father was the british reformed church in the editions of the\n",
      "\n",
      "\n",
      "--- Temperature 1.0 ---\n",
      "FFNN: he was born in it was the us companies operates eva and deemed an cyclones of virtual swamp torpedo portable program the captive is windows linear toward rebel spine condom cuckfield states looking on\n",
      "RNN:  he was born in black and sheen is shown by impulsively dances in using erratic and how the entertainment weekly brutally dance flame disassemble\n",
      "\n",
      "\n",
      "--- Temperature 1.3 ---\n",
      "FFNN: he was born in boise town norman quotes armour solely shelter to stand travel with working behind any choral lewis with controlling either cultural sports also probe economic raid between him which now recurring\n",
      "RNN:  he was born in thomas makes permission again in teaching morphologies well provided skill with consecration including rouen moncrieff leaving ceremonies whose british replied\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_text_generation(ffnn_model, rnn_model, seed_texts, temperatures=[0.7, 1.0, 1.3])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowpy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
